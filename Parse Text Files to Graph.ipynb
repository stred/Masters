{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "_path_to_settings_files = \"C:\\\\Users\\\\Stephen\\\\OneDrive\\\\Documents\\\\NCI\\\\Thesis\\\\Data\"\n",
    "\n",
    "# Path to source files\n",
    "_path_to_files = \"C:\\\\BBC_Data\\\\bbc-fulltext\\\\bbcAll\"\n",
    "#_path_to_files = \"C:\\\\BBC_Data\\\\bbc-sportext\\\\bbcAll\"\n",
    "\n",
    "# Path to outputs\n",
    "_path_to_output = \"C:\\\\BBC_Data\\\\bbc-fulltext\"\n",
    "#_path_to_output = \"C:\\\\BBC_Data\\\\bbc-sportext\"\n",
    "\n",
    "# in_degree or degree\n",
    "_in_degree = True\n",
    "\n",
    "# Window size to use for graph-of-words extraction\n",
    "_windowSize = 3\n",
    "\n",
    "# If the degree is > windowSize-1 then add the term and edge\n",
    "# We use windowSize for in_degree counts, and windowSize*2 for degree counts\n",
    "_degree_compare = (_windowSize-1)*2\n",
    "if _in_degree == True:\n",
    "    _degree_compare = _windowSize-1\n",
    "\n",
    "# Limits for connected term removal\n",
    "_upper_percentile = 99.7\n",
    "_lower_count = 2\n",
    "\n",
    "\n",
    "_Test_Run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "#import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# NetworkX graph library\n",
    "import networkx as nx\n",
    "\n",
    "# Random library\n",
    "import random\n",
    "\n",
    "# File libraries\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Time Library\n",
    "from time import time\n",
    "\n",
    "# Maths library\n",
    "import math\n",
    "\n",
    "# Scipy stats functions\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "# KD Tree algo\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# kmeans cluster algorithm \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Confusion matrix, precision, recall, F1\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Operator\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of Google stopwords from file\n",
    "\n",
    "fname = _path_to_settings_files + \"\\\\GoogleStopwords.txt\"\n",
    "\n",
    "with open(fname) as f:\n",
    "    StopWords = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "StopWords = [x.strip() for x in StopWords]\n",
    "# Stem the stopwords\n",
    "StopWords = list(set([stemmer.stem(y) for y in StopWords]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of stop words to use- first the NLTK stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "# Add in the Google Stopwords\n",
    "cachedStopWords += StopWords\n",
    "#print(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VB style text parsing functions\n",
    "def left(s, amount):\n",
    "    return s[:amount]\n",
    "\n",
    "def right(s, amount):\n",
    "    return s[-amount:]\n",
    "\n",
    "def mid(s, offset, amount):\n",
    "    return s[offset:offset+amount]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick quick brown fox\n"
     ]
    }
   ],
   "source": [
    "# This function cleans a piece of text of non letter/space characters\n",
    "p1 = re.compile(r'[^a-z ]', re.UNICODE)\n",
    "p2 = re.compile(r' +', re.UNICODE)\n",
    "\n",
    "def CleanWord(w):\n",
    "    x = w.lower()\n",
    "    #x = re.sub(r'[^a-z ]','',x)\n",
    "    x = p1.sub(' ', x) #.strip() \n",
    "    x = p2.sub(' ', x).strip()\n",
    "    #x = x.split(' ')\n",
    "    return x\n",
    "\n",
    "print(CleanWord(\"the quick Quick \\r\\nbrown.\\r\\nfox 1.2)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def StoreGraphToFile(Graph, PassNum, Stage, Desc):\n",
    "    passNo = right('00' + str(PassNum), 2)\n",
    "    filename = _path_to_output + '\\\\Pass' + passNo + '.' + str(Stage) + ' ' + Desc + '.csv'\n",
    "    myfile = open(filename, 'w')\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    for n,d in Graph.nodes_iter(data=True):\n",
    "        mylist=str(n) + \"\\t\" + str(d[\"type\"]) + \"\\t\" + str(d[\"x\"]) + \"\\t\" + str(d[\"y\"]) + \"\\n\"\n",
    "        myfile.write(mylist)\n",
    "\n",
    "    myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to parse out words and bigrams\n",
    "def FileToGraph(fileTuple):\n",
    "    # Array to return from function\n",
    "    rArr = []\n",
    "\n",
    "    # Take the filename from the tuple\n",
    "    filename = fileTuple[0]\n",
    "    \n",
    "    # remove non letters from the text in the tuple\n",
    "    r = CleanWord(fileTuple[1])\n",
    "    \n",
    "    # split into an array, stem, and remove words in the stopword list\n",
    "    arr = [stemmer.stem(y) for y in r.split(\" \")]\n",
    "    arr = [word for word in arr if word not in cachedStopWords]\n",
    "    \n",
    "    # Create a new directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    arrLen = len(arr)\n",
    "\n",
    "    # Add the first few nodes\n",
    "    for i in range(0, _windowSize-1):\n",
    "        G.add_node(arr[i])\n",
    "\n",
    "    for i in range(_windowSize-1, arrLen):\n",
    "        G.add_node(arr[i])\n",
    "        for j in range(0, (_windowSize-1)):\n",
    "            src = arr[i-(_windowSize-1)]\n",
    "            tgt = arr[i-j]\n",
    "            if G.has_edge(src, tgt):\n",
    "                # we added this one before, just increase the weight by one\n",
    "                G[src][tgt]['weight'] += 1\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_edge(src, tgt, weight=1)\n",
    "\n",
    "    for i in range(arrLen-(_windowSize-1), arrLen):\n",
    "        for j in range(1, (arrLen-i)):\n",
    "            src = arr[i]\n",
    "            tgt = arr[i+j]\n",
    "            if G.has_edge(src, tgt):\n",
    "                # we added this one before, just increase the weight by one\n",
    "                G[src][tgt]['weight'] += 1\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_edge(src, tgt, weight=1)\n",
    "        \n",
    "    # Get the \"indegree\" or \"degree\" of the terms\n",
    "    if _in_degree == True:\n",
    "        d = G.in_degree()\n",
    "    else:\n",
    "        d = G.degree()\n",
    "    \n",
    "\n",
    "    # Now add all the words the the return array with the filename and the count\n",
    "    for term in d:\n",
    "        rArr.append(filename + \"\\t\" + term + \"\\t\" + str(d[term]))\n",
    "        \n",
    "    # Return the array of graphs and the array of words\n",
    "    return rArr\n",
    "    #return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# LOCAL PYTHON IMPLEMENTATION:\n",
    "def GraphTextFiles(p):\n",
    "\n",
    "    # If this is a folder, we need to append *.* for glob\n",
    "    if(os.path.isdir(p)==True):\n",
    "        p+=\"\\\\*.*\"\n",
    "    \n",
    "    f_list = glob.glob(p)\n",
    "    rVal = []\n",
    "    testSet = []\n",
    "    \n",
    "    # Get all the files - one file at a time\n",
    "    for f in f_list:\n",
    "        contents = Path(f).read_text()\n",
    "        # Hive off a random 10% for a test set\n",
    "        if random.random() < .1:\n",
    "            testSet += FileToGraph((f, contents))\n",
    "        else:\n",
    "            rVal += FileToGraph((f, contents))\n",
    "        \n",
    "    return rVal, testSet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 84.844s.\n"
     ]
    }
   ],
   "source": [
    "##### MAIN PROCESS STARTS HERE #####\n",
    "_Test_Run += 1\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "text_files, test_set = GraphTextFiles(_path_to_files + \"\\\\????.txt\")\n",
    "\n",
    "# PYSPARK IMPLEMENTATION: \n",
    "#text_files = sc.wholeTextFiles(\"C:\\\\BBC_Data\\\\bbc-fulltext\\\\bbcAll\\\\*.txt\") \\\n",
    "#    .flatMap(lambda fileTuple: FileToGraph(fileTuple))\n",
    "#text_files.saveAsTextFile(\"C:\\\\BBC_Data\\\\Output\\\\files\")\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.637140806160406\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set)/len(text_files) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 24.286s.\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "_KL_Limit = 0.0000000250\n",
    "\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "\n",
    "# Create the initial graph\n",
    "Gr=nx.Graph()\n",
    "GTest=nx.Graph()\n",
    "\n",
    "# fArr will hold the array of filenames\n",
    "fArr = []\n",
    "fTest = []\n",
    "\n",
    "# tArr will hold the array of terms added to the graph\n",
    "tArr = []\n",
    "tTest = []\n",
    "\n",
    "# tArr1 will hold the array of terms that we have seen at least once\n",
    "tArr1 = []\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "for x in text_files:   # FOR SPARK APPEND: .collect():\n",
    "\n",
    "    # Split out the \n",
    "    y = x.split(\"\\t\")\n",
    "    filename = y[0]\n",
    "    term = y[1]\n",
    "    degree = y[2]\n",
    "    label = filename[32:33]\n",
    "    \n",
    "    # If the filename is not already added, add it to the graph\n",
    "    if not filename in fArr:\n",
    "\n",
    "        # Create some random spatial positions\n",
    "        xR = random.random()*1000\n",
    "        yR = random.random()*1000\n",
    "        \n",
    "        # Add the file node to the graph\n",
    "        Gr.add_node(filename, type=\"file\", x=xR, y = yR, label = label)\n",
    "        \n",
    "        # Add the filename to the file array\n",
    "        fArr.append(filename)\n",
    "        \n",
    "    # if the degree is over the limit\n",
    "    if int(degree)>_degree_compare:\n",
    "        # If we have not seen this term before, add a node to the graph\n",
    "        if not term in tArr:\n",
    "            # Create some random spatial positions\n",
    "            xR2 = random.random()*1000\n",
    "            yR2 = random.random()*1000\n",
    "\n",
    "            # Add the term to the graph\n",
    "            Gr.add_node(term, type=\"term\", x=xR2, y=yR2)\n",
    "            tArr.append(term)\n",
    "            \n",
    "        # Add the edge to the graph\n",
    "        Gr.add_edge(filename, term, weight=degree)\n",
    "\n",
    "for x in test_set:\n",
    "\n",
    "    # Split out the \n",
    "    y = x.split(\"\\t\")\n",
    "    filename = y[0]\n",
    "    term = y[1]\n",
    "    degree = y[2]\n",
    "    label = filename[32:33]\n",
    "    \n",
    "    # If the filename is not already added, add it to the test Graph\n",
    "    if not filename in fTest:\n",
    "\n",
    "        # Add the file node to the graph\n",
    "        GTest.add_node(filename, type=\"file\", label = label)\n",
    "        \n",
    "        # Add the filename to the file array\n",
    "        fTest.append(filename)\n",
    "        \n",
    "    # if the degree is over the limit\n",
    "    if int(degree)>_degree_compare:\n",
    "        # If we have not seen this term before, add a node to the graph\n",
    "        if not term in tTest:\n",
    "            # Add the term to the graph\n",
    "            GTest.add_node(term, type=\"term\")\n",
    "            tTest.append(term)\n",
    "            \n",
    "        # Add the edge to the graph\n",
    "        GTest.add_edge(filename, term, weight=degree)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.479s.\n"
     ]
    }
   ],
   "source": [
    "# Remove terms with only one connection or > % fractile # of connections\n",
    "t0 = time()\n",
    "#d = Gr.in_degree()\n",
    "d = Gr.degree()\n",
    "\n",
    "## Store degrees to file\n",
    "#filename = _path_to_output + '\\\\term_degree.csv'\n",
    "#myfile = open(filename, 'w')\n",
    "#wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# List to allow calculation of percentile\n",
    "degreeList = []\n",
    "\n",
    "for t in tArr:\n",
    "    degreeList.append(d[t])\n",
    "    #myfile.write(t + \"\\t\" + str(d[t]) + \"\\n\")\n",
    "\n",
    "\n",
    "#myfile.close()\n",
    "\n",
    "# What #degree is > 99.7    \n",
    "perc = np.percentile(degreeList, _upper_percentile)\n",
    "\n",
    "for t in tArr:\n",
    "    n = d[t]\n",
    "    # Remove <2 #####or > 99.7 pecentile\n",
    "    if n < _lower_count or n > perc:\n",
    "        # Remove Edge\n",
    "        for f in Gr.neighbors(t):\n",
    "            Gr.remove_edge(f,t)\n",
    "        # Remove node\n",
    "        Gr.remove_node(t)\n",
    "        # Remove from tArr list\n",
    "        tArr.remove(t)\n",
    "        \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.003s.\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries for calculating Kullback-Leibler\n",
    "Px = []\n",
    "Qx = []\n",
    "Py = []\n",
    "Qy = []\n",
    "\n",
    "# Load the Q arrays with the x/y probabilities\n",
    "t0 = time()\n",
    "for f in fArr:\n",
    "    Qx.append(Gr.node[f][\"x\"])\n",
    "    Qy.append(Gr.node[f][\"y\"])\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.290765119235, 0.238928124581\n",
      "1: 0.0028378821003, 0.00235341428734\n",
      "2: 0.000213198224923, 0.000178912570858\n",
      "3: 2.87776315018e-05, 2.42051208815e-05\n",
      "4: 5.90806078799e-06, 4.58296935101e-06\n",
      "5: 1.68162903861e-06, 1.06799920528e-06\n",
      "6: 5.96757365842e-07, 2.84712362614e-07\n",
      "7: 2.40682643492e-07, 8.3806510889e-08\n",
      "8: 1.04105181901e-07, 2.68687805488e-08\n",
      "9: 4.68213118815e-08, 9.34249975797e-09\n",
      "10: 2.15492334226e-08, 3.51067450924e-09\n",
      "11: 1.0063539824e-08, 1.41507928292e-09\n",
      "done in 7.077s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initial values for Kullback-Leibler\n",
    "KLx = 999\n",
    "KLy = 999\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Run several itterations of moving points\n",
    "#for i in range(0, 20):\n",
    "i = 0\n",
    "#while KLx + KLy > 0.0000000125:\n",
    "while KLx + KLy > _KL_Limit:\n",
    "    \n",
    "    # Reposition all the term nodes in the centroid of their documents\n",
    "    for t in tArr:\n",
    "        sumx=0\n",
    "        sumy=0\n",
    "        sumw=0\n",
    "\n",
    "        for f in Gr.neighbors(t):\n",
    "            w=float(Gr.edge[f][t][\"weight\"])\n",
    "            sumw+=w\n",
    "            sumx+=float(Gr.node[f][\"x\"])*w\n",
    "            sumy+=float(Gr.node[f][\"y\"])*w\n",
    "\n",
    "        if sumw > 0:\n",
    "            Gr.node[t][\"x\"] = sumx/sumw\n",
    "            Gr.node[t][\"y\"] = sumy/sumw\n",
    "        else:\n",
    "            print(\"0 weight on term \" + t)\n",
    "\n",
    "    StoreGraphToFile(Gr, i, 1, \"Reposition Terms\")\n",
    "    \n",
    "    pSumx = 0\n",
    "    pSumy = 0\n",
    "    \n",
    "    # Reposition all the file nodes in the centroid of their terms\n",
    "    for f in fArr:\n",
    "        sumx=0\n",
    "        sumy=0\n",
    "        sumw=0\n",
    "\n",
    "        for t in Gr.neighbors(f):\n",
    "            w=float(Gr.edge[f][t][\"weight\"])\n",
    "            sumw+=w\n",
    "            sumx+=float(Gr.node[t][\"x\"])*w\n",
    "            sumy+=float(Gr.node[t][\"y\"])*w\n",
    "\n",
    "        if sumw > 0:\n",
    "            Gr.node[f][\"x\"] = sumx/sumw\n",
    "            pSumx += sumx/sumw\n",
    "            Gr.node[f][\"y\"] = sumy/sumw\n",
    "            pSumy += sumy/sumw\n",
    "        else:\n",
    "            print(\"0 weight on file \" + f)\n",
    "\n",
    "    # Load the P arrays with the x/y probabilities\n",
    "    # and calculate the KL sum\n",
    "    for f in fArr:\n",
    "        Px.append(Gr.node[f][\"x\"])\n",
    "        Py.append(Gr.node[f][\"y\"])\n",
    "        #print(\"a\" if x < y else \"b\")\n",
    "        #KLx += ((Px[f] * math.log(Px[f]/Qx[f]))+(Qx[f] * math.log(Qx[f]/Px[f]))) if Px[f] > 0 else 0\n",
    "        #KLy += ((Py[f] * math.log(Py[f]/Qy[f]))+(Qy[f] * math.log(Qy[f]/Py[f]))) if Py[f] > 0 else 0\n",
    "\n",
    "    KLx = scipy.stats.entropy(Px, Qx)\n",
    "    KLy = scipy.stats.entropy(Py, Qy)\n",
    "\n",
    "    StoreGraphToFile(Gr, i, 2, \"Reposition Files\")\n",
    "    \n",
    "    # Assign the P sets to the Q sets for the next run\n",
    "    Qx = Px\n",
    "    Qy = Py\n",
    "    Px = []\n",
    "    Py = []\n",
    "    \n",
    "    print(str(i) + \": \" + str(KLx) + \", \" + str(KLy))\n",
    "    i+=1\n",
    "\n",
    "numIter = i-1\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract values for Tree from Graph\n",
    "Y = [];\n",
    "Y_label = [];\n",
    "\n",
    "for f in fArr:\n",
    "    x = Gr.node[f][\"x\"]\n",
    "    y = Gr.node[f][\"y\"]\n",
    "    lab = Gr.node[f][\"label\"]\n",
    "    Y.append([x,y])\n",
    "    Y_label.append(lab)\n",
    "\n",
    "tree = KDTree(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written in 106.546s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Write test results to a file\n",
    "filename = 'C:\\\\BBC_Data\\\\Pred9_News_' + str(int(_LK_Limit*10000000000)) + '_' + str(numIter) + '_' + str(_Test_Run) + '.txt'\n",
    "myfile = open(filename, 'w')\n",
    "wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "for test_k in range(1,100):\n",
    "    Actual = []\n",
    "    Pred = []\n",
    "\n",
    "    for f in fTest:\n",
    "        sumx=0\n",
    "        sumy=0\n",
    "        sumw=0\n",
    "        # What is the test set label?\n",
    "        lab=GTest.node[f][\"label\"]\n",
    "        pred_lab = dict()\n",
    "\n",
    "        for t in GTest.neighbors(f):\n",
    "            # is this test term in our term array?\n",
    "            if t in tArr:\n",
    "                # Get the weight of the edge from the test graph\n",
    "                w=float(GTest.edge[f][t][\"weight\"])\n",
    "                sumw+=w\n",
    "                # Get the x,y of the term from the model graph\n",
    "                sumx+=float(Gr.node[t][\"x\"])*w\n",
    "                sumy+=float(Gr.node[t][\"y\"])*w\n",
    "\n",
    "        if sumw > 0:\n",
    "            # Get the centroid\n",
    "            x = sumx/sumw\n",
    "            y = sumy/sumw\n",
    "            # Query the tree for nearest neighbor\n",
    "            dist,ind = tree.query([[x,y]], k=test_k)\n",
    "            for i in ind[0]:\n",
    "                if Y_label[i] in pred_lab:\n",
    "                    pred_lab[Y_label[i]] += 1\n",
    "                else:\n",
    "                    pred_lab[Y_label[i]] = 1\n",
    "\n",
    "        else:\n",
    "            pred_lab[\"UNK\"] = 1\n",
    "\n",
    "        #print(f + \": \" + lab + \", pred=\" + str(sorted(pred_lab.items(), key=operator.itemgetter(1), reverse=True)[0][0]))\n",
    "        Actual.append(lab)\n",
    "        Pred.append(str(sorted(pred_lab.items(), key=operator.itemgetter(1), reverse=True)[0][0]))\n",
    "\n",
    "    precision = precision_score(Actual, Pred, average='weighted') \n",
    "    recall = recall_score(Actual, Pred, average='weighted')\n",
    "    f1 = f1_score(Actual, Pred, average='weighted') \n",
    "    #print(str(test_k) + ',' + str(precision) + ',' + str(recall) + ',' + str(f1))\n",
    "    \n",
    "    myfile.write(str(test_k) + ',' + str(precision) + ',' + str(recall) + ',' + str(f1) +'\\n')\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "print(\"File written in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "WAIT HERE\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-705-d230bab182e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'dummy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mftg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileToGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0msumx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msumy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-298-c034e6c261e5>\u001b[0m in \u001b[0;36mFileToGraph\u001b[1;34m(fileTuple)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Add the first few nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_windowSize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_windowSize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Specify K to use for this test\n",
    "test_k = 35\n",
    "\n",
    "text = input()\n",
    "\n",
    "#text = 'in natural language processing (NLP) a text graph is a graph representation of a text item '\n",
    "#text += '(document, passage or sentence) it is typically created as a preprocessing step to support '\n",
    "#text += 'NLP tasks such as text condensation term disambiguation (topic based) text summarization '\n",
    "#text += '(summarize large text collections) and relation extraction (extract relations from unstructured text)'\n",
    "d = ['dummy', text]\n",
    "\n",
    "ftg = FileToGraph(d)\n",
    "sumx=0\n",
    "sumy=0\n",
    "sumw=0\n",
    "for res in ftg: \n",
    "    tm = res.split('\\t')[1]\n",
    "    wt = int(res.split('\\t')[2])\n",
    "    \n",
    "    \n",
    "    # Only include keywords\n",
    "    if tm in tArr and wt>_degree_compare:\n",
    "        sumw+=wt\n",
    "        # Get the x,y of the term from the model graph\n",
    "        x = Gr.node[tm][\"x\"]\n",
    "        y = Gr.node[tm][\"y\"]\n",
    "\n",
    "        print(tm + ': ' + str(wt) + ' (' + str(x) + ',' + str(y) + ')')\n",
    "\n",
    "        sumx+=float(Gr.node[tm][\"x\"])*wt\n",
    "        sumy+=float(Gr.node[tm][\"y\"])*wt\n",
    "\n",
    "if sumw > 0:\n",
    "    # Get the centroid\n",
    "    x = sumx/sumw\n",
    "    y = sumy/sumw\n",
    "    print('Centroid: (' + str(x) + ',' + str(y) + ')')\n",
    "    # Query the tree for nearest neighbor\n",
    "    dist,ind = tree.query([[x,y]], k=test_k)\n",
    "    for i in ind[0]:\n",
    "        if Y_label[i] in pred_lab:\n",
    "            pred_lab[Y_label[i]] += 1\n",
    "        else:\n",
    "            pred_lab[Y_label[i]] = 1\n",
    "\n",
    "else:\n",
    "    pred_lab[\"UNK\"] = 1\n",
    "\n",
    "print('Prediction: ' + str(pred_lab))\n",
    "#str(sorted(pred_lab.items(), key=operator.itemgetter(1), reverse=True)[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### CLUSTERING\n",
    "\n",
    "\n",
    "# Change text label to number\n",
    "def strToNum(str):\n",
    "    rVal = 0\n",
    "    if str == \"B\":\n",
    "        rVal = 0\n",
    "    elif str == \"E\":\n",
    "        rVal = 1\n",
    "    elif str == \"P\":\n",
    "        rVal = 2\n",
    "    elif str == \"S\":\n",
    "        rVal = 3\n",
    "    elif str == \"T\":\n",
    "        rVal = 4\n",
    "        \n",
    "    return rVal\n",
    "\n",
    "\n",
    "# Load the data from the graph into a dataframe\n",
    "df = pd.DataFrame(columns=['filename','x','y','Label'])\n",
    "df.Label = df.Label.astype(int)\n",
    "\n",
    "for f in fArr:\n",
    "    s = strToNum(f[32:33])\n",
    "    df = df.append({\"filename\": f, \"x\":Gr.node[f][\"x\"], \"y\":Gr.node[f][\"y\"], \"Label\": s}, ignore_index=True)\n",
    "\n",
    "#df = df.append({\"filename\": \"file1\", \"x\":1, \"y\":2}, ignore_index=True)\n",
    "#df = df.append({\"filename\": \"file2\", \"x\":2, \"y\":3}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster the documents using kmeans\n",
    "kmeans = KMeans(n_clusters=5,init='k-means++')\n",
    "result = kmeans.fit_predict(df[[\"x\",\"y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"Cluster\"]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df.Label, df.Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[213,   0,   0,   2, 295],\n",
       "       [ 79,   2,   0, 293,  12],\n",
       "       [402,   0,   1,   4,  10],\n",
       "       [ 47,   1, 457,   6,   0],\n",
       "       [ 81,   0,   0,  12, 308]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tc = pd.DataFrame(columns=['term','weight','Cluster'])\n",
    "tc.Cluster = tc.Cluster.astype(int)\n",
    "\n",
    "for f in fArr:\n",
    "    dftemp=df[df['filename'] == f]\n",
    "    c = dftemp.get_value(dftemp.index[0], col='Cluster')\n",
    "    for t in Gr.neighbors(f):\n",
    "        w=float(Gr.edge[f][t][\"weight\"])\n",
    "        tc = tc.append({\"term\": t, \"weight\": w, \"Cluster\": c}, ignore_index=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "numIter = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
