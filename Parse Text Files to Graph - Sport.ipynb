{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "#import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# NetworkX graph library\n",
    "import networkx as nx\n",
    "\n",
    "# Random library\n",
    "import random\n",
    "\n",
    "# File libraries\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Time Library\n",
    "from time import time\n",
    "\n",
    "# Maths library\n",
    "import math\n",
    "\n",
    "# Scipy stats functions\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "# KD Tree algo\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# kmeans cluster algorithm \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Confusion matrix, precision, recall, F1\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Operator\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of Google stopwords from file\n",
    "\n",
    "fname = \"C:\\\\Users\\\\Stephen\\\\OneDrive\\\\Documents\\\\NCI\\\\Thesis\\\\Data\\\\GoogleStopwords.txt\"\n",
    "\n",
    "with open(fname) as f:\n",
    "    StopWords = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "StopWords = [x.strip() for x in StopWords]\n",
    "# Stem the stopwords\n",
    "StopWords = list(set([stemmer.stem(y) for y in StopWords]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of stop words to use- first the NLTK stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "# Add in the Google Stopwords\n",
    "cachedStopWords += StopWords\n",
    "#print(cachedStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VB style text parsing functions\n",
    "def left(s, amount):\n",
    "    return s[:amount]\n",
    "\n",
    "def right(s, amount):\n",
    "    return s[-amount:]\n",
    "\n",
    "def mid(s, offset, amount):\n",
    "    return s[offset:offset+amount]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick quick brown fox\n"
     ]
    }
   ],
   "source": [
    "# This function cleans a piece of text of non letter/space characters\n",
    "p1 = re.compile(r'[^a-z ]', re.UNICODE)\n",
    "p2 = re.compile(r' +', re.UNICODE)\n",
    "\n",
    "def CleanWord(w):\n",
    "    x = w.lower()\n",
    "    #x = re.sub(r'[^a-z ]','',x)\n",
    "    x = p1.sub(' ', x) #.strip() \n",
    "    x = p2.sub(' ', x).strip()\n",
    "    #x = x.split(' ')\n",
    "    return x\n",
    "\n",
    "print(CleanWord(\"the quick Quick \\r\\nbrown.\\r\\nfox 1.2)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def StoreGraphToFile(Graph, PassNum, Stage, Desc):\n",
    "    passNo = right('00' + str(PassNum), 2)\n",
    "    filename = 'C:\\\\BBC_Data\\\\bbc-sportext\\\\Pass' + passNo + '.' + str(Stage) + ' ' + Desc + '.csv'\n",
    "    myfile = open(filename, 'w')\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    for n,d in Graph.nodes_iter(data=True):\n",
    "        mylist=str(n) + \"\\t\" + str(d[\"type\"]) + \"\\t\" + str(d[\"x\"]) + \"\\t\" + str(d[\"y\"]) + \"\\n\"\n",
    "        myfile.write(mylist)\n",
    "\n",
    "    myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file://file/name.ext\tstep\t2\n",
      "file://file/name.ext\tlarg\t1\n",
      "file://file/name.ext\tdisambigu\t2\n",
      "file://file/name.ext\tcreat\t2\n",
      "file://file/name.ext\tnlp\t4\n",
      "file://file/name.ext\ttopic\t2\n",
      "file://file/name.ext\tcollect\t2\n",
      "file://file/name.ext\tnatur\t0\n",
      "file://file/name.ext\ttypic\t2\n",
      "file://file/name.ext\tdocument\t2\n",
      "file://file/name.ext\tpreprocess\t2\n",
      "file://file/name.ext\textract\t3\n",
      "file://file/name.ext\trepresent\t1\n",
      "file://file/name.ext\tsentenc\t2\n",
      "file://file/name.ext\ttask\t2\n",
      "file://file/name.ext\titem\t2\n",
      "file://file/name.ext\tcondens\t2\n",
      "file://file/name.ext\tterm\t2\n",
      "file://file/name.ext\tunstructur\t2\n",
      "file://file/name.ext\tprocess\t2\n",
      "file://file/name.ext\tsummar\t3\n",
      "file://file/name.ext\tgraph\t3\n",
      "file://file/name.ext\trelat\t3\n",
      "file://file/name.ext\tsupport\t2\n",
      "file://file/name.ext\tlanguag\t1\n",
      "file://file/name.ext\ttext\t11\n",
      "file://file/name.ext\tpassag\t2\n",
      "file://file/name.ext\tbase\t2\n"
     ]
    }
   ],
   "source": [
    "# Window size to use for graph-of-words extraction\n",
    "windowSize = 3\n",
    "\n",
    "\n",
    "# Function to parse out words and bigrams\n",
    "def FileToGraph(fileTuple):\n",
    "    # Array to return from function\n",
    "    rArr = []\n",
    "\n",
    "    # Take the filename from the tuple\n",
    "    filename = fileTuple[0]\n",
    "    \n",
    "    # remove non letters from the text in the tuple\n",
    "    r = CleanWord(fileTuple[1])\n",
    "    \n",
    "    # split into an array, stem, and remove words in the stopword list\n",
    "    arr = [stemmer.stem(y) for y in r.split(\" \")]\n",
    "    arr = [word for word in arr if word not in cachedStopWords]\n",
    "    \n",
    "    # Create a new directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    arrLen = len(arr)\n",
    "\n",
    "    # Add the first few nodes\n",
    "    for i in range(0, windowSize-1):\n",
    "        G.add_node(arr[i])\n",
    "\n",
    "    for i in range(windowSize-1, arrLen):\n",
    "        G.add_node(arr[i])\n",
    "        for j in range(0, (windowSize-1)):\n",
    "            src = arr[i-(windowSize-1)]\n",
    "            tgt = arr[i-j]\n",
    "            if G.has_edge(src, tgt):\n",
    "                # we added this one before, just increase the weight by one\n",
    "                G[src][tgt]['weight'] += 1\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_edge(src, tgt, weight=1)\n",
    "\n",
    "    for i in range(arrLen-(windowSize-1), arrLen):\n",
    "        for j in range(1, (arrLen-i)):\n",
    "            src = arr[i]\n",
    "            tgt = arr[i+j]\n",
    "            if G.has_edge(src, tgt):\n",
    "                # we added this one before, just increase the weight by one\n",
    "                G[src][tgt]['weight'] += 1\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_edge(src, tgt, weight=1)\n",
    "        \n",
    "    # Get the \"indegree\" of the terms\n",
    "    #d = G.degree()\n",
    "    d = G.in_degree()\n",
    "\n",
    "    # Now add all the words the the return array with the filename and the count\n",
    "    for term in d:\n",
    "        rArr.append(filename + \"\\t\" + term + \"\\t\" + str(d[term]))\n",
    "        \n",
    "    # Return the array of graphs and the array of words\n",
    "    return rArr\n",
    "    #return G\n",
    "\n",
    "text = 'in natural language processing (NLP) a text graph is a graph representation of a text item '\n",
    "text += '(document, passage or sentence) it is typically created as a preprocessing step to support '\n",
    "text += 'NLP tasks such as text condensation term disambiguation (topic based) text summarization '\n",
    "text += '(summarize large text collections) and relation extraction (extract relations from unstructured text)'\n",
    "d = ['file://file/name.ext', text]\n",
    "\n",
    "x = FileToGraph(d)\n",
    "for y in x: #.edges():\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 24.203s.\n"
     ]
    }
   ],
   "source": [
    "# Get all the files - one file at a time\n",
    "t0 = time()\n",
    "\n",
    "# LOCAL PYTHON IMPLEMENTATION:\n",
    "def GraphTextFiles(p):\n",
    "    # If this is a folder, we need to append *.* for glob\n",
    "    if(os.path.isdir(p)==True):\n",
    "        p+=\"\\\\*.*\"\n",
    "    \n",
    "    f_list = glob.glob(p)\n",
    "    rVal = []\n",
    "    testSet = []\n",
    "    \n",
    "    for f in f_list:\n",
    "        contents = Path(f).read_text()\n",
    "        # Hive off a random 10% for a test set\n",
    "        if random.random() < .1:\n",
    "            testSet += FileToGraph((f, contents))\n",
    "        else:\n",
    "            rVal += FileToGraph((f, contents))\n",
    "        \n",
    "    return rVal, testSet\n",
    "\n",
    "text_files, test_set = GraphTextFiles(\"C:\\\\BBC_Data\\\\bbc-sportext\\\\bbcAll\\\\????.txt\")\n",
    "\n",
    "## Write initial files and term weights to file\n",
    "#filename = 'C:\\\\BBC_Data\\\\bbc-fulltext\\\\Initial_Graph_Text.csv'\n",
    "#myfile = open(filename, 'w')\n",
    "#wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#\n",
    "#for x in text_files:\n",
    "#    myfile.write(x + \"\\n\")\n",
    "#\n",
    "#myfile.close()\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# PYSPARK IMPLEMENTATION: \n",
    "#text_files = sc.wholeTextFiles(\"C:\\\\BBC_Data\\\\bbc-fulltext\\\\bbcAll\\\\*.txt\") \\\n",
    "#    .flatMap(lambda fileTuple: FileToGraph(fileTuple))\n",
    "#text_files.saveAsTextFile(\"C:\\\\BBC_Data\\\\Output\\\\files\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.260207373617632\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set)/len(text_files) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.119s.\n"
     ]
    }
   ],
   "source": [
    "# Create the initial graph\n",
    "Gr=nx.Graph()\n",
    "GTest=nx.Graph()\n",
    "\n",
    "# fArr will hold the array of filenames\n",
    "fArr = []\n",
    "fTest = []\n",
    "\n",
    "# tArr will hold the array of terms added to the graph\n",
    "tArr = []\n",
    "tTest = []\n",
    "\n",
    "# tArr1 will hold the array of terms that we have seen at least once\n",
    "tArr1 = []\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "for x in text_files:   # FOR SPARK APPEND: .collect():\n",
    "\n",
    "    # Split out the \n",
    "    y = x.split(\"\\t\")\n",
    "    filename = y[0]\n",
    "    term = y[1]\n",
    "    degree = y[2]\n",
    "    label = filename[32:33]\n",
    "    \n",
    "    # If the filename is not already added, add it to the graph\n",
    "    if not filename in fArr:\n",
    "\n",
    "        # Create some random spatial positions\n",
    "        xR = random.random()*1000\n",
    "        yR = random.random()*1000\n",
    "        \n",
    "        # Add the file node to the graph\n",
    "        Gr.add_node(filename, type=\"file\", x=xR, y = yR, label = label)\n",
    "        \n",
    "        # Add the filename to the file array\n",
    "        fArr.append(filename)\n",
    "        \n",
    "    # If the degree is > windowSize-1 then add the term and edge\n",
    "    # We use windowSize for in_degree counts, and windowSize*2 for degree counts\n",
    "    #if int(degree)>((windowSize-1)*2):\n",
    "    if int(degree)>(windowSize-1):\n",
    "        # If we have not seen this term before, add a node to the graph\n",
    "        if not term in tArr:\n",
    "            # Create some random spatial positions\n",
    "            xR2 = random.random()*1000\n",
    "            yR2 = random.random()*1000\n",
    "\n",
    "            # Add the term to the graph\n",
    "            Gr.add_node(term, type=\"term\", x=xR2, y=yR2)\n",
    "            tArr.append(term)\n",
    "            \n",
    "        # Add the edge to the graph\n",
    "        Gr.add_edge(filename, term, weight=degree)\n",
    "\n",
    "for x in test_set:\n",
    "\n",
    "    # Split out the \n",
    "    y = x.split(\"\\t\")\n",
    "    filename = y[0]\n",
    "    term = y[1]\n",
    "    degree = y[2]\n",
    "    label = filename[32:33]\n",
    "    \n",
    "    # If the filename is not already added, add it to the test Graph\n",
    "    if not filename in fTest:\n",
    "\n",
    "        # Add the file node to the graph\n",
    "        GTest.add_node(filename, type=\"file\", label = label)\n",
    "        \n",
    "        # Add the filename to the file array\n",
    "        fTest.append(filename)\n",
    "        \n",
    "    # If the degree is > windowSize-1 then add the term and edge\n",
    "    # We use windowSize for in_degree counts, and windowSize*2 for degree counts\n",
    "    #if int(degree)>((windowSize-1)*2):\n",
    "    if int(degree)>(windowSize-1):\n",
    "        # If we have not seen this term before, add a node to the graph\n",
    "        if not term in tTest:\n",
    "            # Add the term to the graph\n",
    "            GTest.add_node(term, type=\"term\")\n",
    "            tTest.append(term)\n",
    "            \n",
    "        # Add the edge to the graph\n",
    "        GTest.add_edge(filename, term, weight=degree)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List size: 3080\n",
      "Perc: 107.815\n",
      "done in 0.090s.\n"
     ]
    }
   ],
   "source": [
    "# Remove terms with only one connection or > 95% fractile # of connections\n",
    "t0 = time()\n",
    "#d = Gr.in_degree()\n",
    "d = Gr.degree()\n",
    "\n",
    "# Store degrees to file\n",
    "filename = 'C:\\\\BBC_Data\\\\bbc-sportext\\\\term_degree.csv'\n",
    "myfile = open(filename, 'w')\n",
    "wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# List to allow calculation of percentile\n",
    "degreeList = []\n",
    "\n",
    "for t in tArr:\n",
    "    myfile.write(t + \"\\t\" + str(d[t]) + \"\\n\")\n",
    "    degreeList.append(d[t])\n",
    "\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "    \n",
    "print(\"List size: \" + str(len(degreeList)))\n",
    "perc = np.percentile(degreeList, 99.7)\n",
    "print(\"Perc: \" + str(perc))\n",
    "\n",
    "for t in tArr:\n",
    "    n = d[t]\n",
    "    #print(n)\n",
    "    if n < 2 or n > perc:\n",
    "        # Remove Edge\n",
    "        for f in Gr.neighbors(t):\n",
    "            Gr.remove_edge(f,t)\n",
    "        # Remove node\n",
    "        Gr.remove_node(t)\n",
    "        # Remove from tArr list\n",
    "        tArr.remove(t)\n",
    "        \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.002s.\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries for calculating Kullback-Leibler\n",
    "Px = []\n",
    "Qx = []\n",
    "Py = []\n",
    "Qy = []\n",
    "\n",
    "# Load the Q arrays with the x/y probabilities\n",
    "t0 = time()\n",
    "for f in fArr:\n",
    "    Qx.append(Gr.node[f][\"x\"])\n",
    "    Qy.append(Gr.node[f][\"y\"])\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.240785566422, 0.228771574523\n",
      "1: 0.0036341520349, 0.00344722509395\n",
      "2: 0.000366987403635, 0.000340710369009\n",
      "3: 6.22740630181e-05, 5.44596024961e-05\n",
      "4: 1.41024392476e-05, 1.14860752237e-05\n",
      "5: 3.89575463025e-06, 2.95207900108e-06\n",
      "6: 1.24465848125e-06, 8.81089344457e-07\n",
      "7: 4.41193681903e-07, 2.9437060077e-07\n",
      "8: 1.68017419056e-07, 1.06986519152e-07\n",
      "9: 6.72044535995e-08, 4.13827793886e-08\n",
      "10: 2.78210649765e-08, 1.67573393302e-08\n",
      "11: 1.18110090831e-08, 7.0175434077e-09\n",
      "12: 5.11254276848e-09, 3.01242807218e-09\n",
      "done in 2.452s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "KLx = 999\n",
    "KLy = 999\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Run several itterations of moving points\n",
    "#for i in range(0, 20):\n",
    "i = 0\n",
    "while KLx + KLy > 0.0000000125:\n",
    "    \n",
    "    # Reposition all the term nodes in the centroid of their documents\n",
    "    for t in tArr:\n",
    "        sumx=0\n",
    "        sumy=0\n",
    "        sumw=0\n",
    "\n",
    "        for f in Gr.neighbors(t):\n",
    "            w=float(Gr.edge[f][t][\"weight\"])\n",
    "            sumw+=w\n",
    "            sumx+=float(Gr.node[f][\"x\"])*w\n",
    "            sumy+=float(Gr.node[f][\"y\"])*w\n",
    "\n",
    "        if sumw > 0:\n",
    "            Gr.node[t][\"x\"] = sumx/sumw\n",
    "            Gr.node[t][\"y\"] = sumy/sumw\n",
    "        else:\n",
    "            print(\"0 weight on term \" + t)\n",
    "\n",
    "    StoreGraphToFile(Gr, i, 1, \"Reposition Terms\")\n",
    "    \n",
    "    pSumx = 0\n",
    "    pSumy = 0\n",
    "    \n",
    "    # Reposition all the file nodes in the centroid of their terms\n",
    "    for f in fArr:\n",
    "        sumx=0\n",
    "        sumy=0\n",
    "        sumw=0\n",
    "\n",
    "        for t in Gr.neighbors(f):\n",
    "            w=float(Gr.edge[f][t][\"weight\"])\n",
    "            sumw+=w\n",
    "            sumx+=float(Gr.node[t][\"x\"])*w\n",
    "            sumy+=float(Gr.node[t][\"y\"])*w\n",
    "\n",
    "        if sumw > 0:\n",
    "            Gr.node[f][\"x\"] = sumx/sumw\n",
    "            pSumx += sumx/sumw\n",
    "            Gr.node[f][\"y\"] = sumy/sumw\n",
    "            pSumy += sumy/sumw\n",
    "        else:\n",
    "            print(\"0 weight on file \" + f)\n",
    "\n",
    "    # Load the P arrays with the x/y probabilities\n",
    "    # and calculate the KL sum\n",
    "    for f in fArr:\n",
    "        Px.append(Gr.node[f][\"x\"])\n",
    "        Py.append(Gr.node[f][\"y\"])\n",
    "        #print(\"a\" if x < y else \"b\")\n",
    "        #KLx += ((Px[f] * math.log(Px[f]/Qx[f]))+(Qx[f] * math.log(Qx[f]/Px[f]))) if Px[f] > 0 else 0\n",
    "        #KLy += ((Py[f] * math.log(Py[f]/Qy[f]))+(Qy[f] * math.log(Qy[f]/Py[f]))) if Py[f] > 0 else 0\n",
    "\n",
    "    KLx = scipy.stats.entropy(Px, Qx)\n",
    "    KLy = scipy.stats.entropy(Py, Qy)\n",
    "\n",
    "    StoreGraphToFile(Gr, i, 2, \"Reposition Files\")\n",
    "    \n",
    "    # Assign the P sets to the Q sets for the next run\n",
    "    Qx = Px\n",
    "    Qy = Py\n",
    "    Px = []\n",
    "    Py = []\n",
    "    \n",
    "    print(str(i) + \": \" + str(KLx) + \", \" + str(KLy))\n",
    "    i+=1\n",
    "    \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract values for Tree from Graph\n",
    "Y = [];\n",
    "Y_label = [];\n",
    "\n",
    "for f in fArr:\n",
    "    x = Gr.node[f][\"x\"]\n",
    "    y = Gr.node[f][\"y\"]\n",
    "    lab = Gr.node[f][\"label\"]\n",
    "    Y.append([x,y])\n",
    "    Y_label.append(lab)\n",
    "\n",
    "tree = KDTree(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0.778458372903,0.777777777778,0.767685768855\n",
      "2,0.81899172626,0.79012345679,0.767890741277\n",
      "3,0.84988423592,0.851851851852,0.850119791406\n",
      "4,0.821214825769,0.827160493827,0.821082383071\n",
      "5,0.815789224729,0.814814814815,0.814097616559\n",
      "6,0.815801695112,0.827160493827,0.819486989078\n",
      "7,0.796722009366,0.802469135802,0.79934403911\n",
      "8,0.83567251462,0.83950617284,0.836611947723\n",
      "9,0.814814814815,0.814814814815,0.814814814815\n",
      "10,0.81794455274,0.814814814815,0.815653072149\n",
      "11,0.826279017354,0.827160493827,0.825040397631\n",
      "12,0.819590643275,0.827160493827,0.822195903027\n",
      "13,0.821623643846,0.827160493827,0.822620087895\n",
      "14,0.807793927845,0.814814814815,0.810406398411\n",
      "15,0.839096464021,0.83950617284,0.838512729695\n",
      "16,0.811804201863,0.814814814815,0.812581458558\n",
      "17,0.805367494137,0.814814814815,0.808349026993\n",
      "18,0.80548696845,0.814814814815,0.809067688378\n",
      "19,0.8109739369,0.814814814815,0.811991664986\n",
      "20,0.821149748126,0.827160493827,0.822797541442\n",
      "21,0.821149748126,0.827160493827,0.822797541442\n",
      "22,0.807569501212,0.814814814815,0.809863972952\n",
      "23,0.786200515711,0.79012345679,0.785704452594\n",
      "24,0.806739236249,0.814814814815,0.80927417938\n",
      "25,0.807446600039,0.814814814815,0.809901391291\n",
      "26,0.794693523933,0.802469135802,0.797920399453\n",
      "27,0.807446600039,0.814814814815,0.809901391291\n",
      "28,0.807446600039,0.814814814815,0.809901391291\n",
      "29,0.807446600039,0.814814814815,0.809901391291\n",
      "30,0.807446600039,0.814814814815,0.809901391291\n",
      "31,0.807446600039,0.814814814815,0.809901391291\n",
      "32,0.807446600039,0.814814814815,0.809901391291\n",
      "33,0.817283950617,0.827160493827,0.819928172923\n",
      "34,0.824916139295,0.827160493827,0.824224820819\n",
      "35,0.834616315662,0.83950617284,0.834545547189\n",
      "36,0.834616315662,0.83950617284,0.834545547189\n",
      "37,0.834616315662,0.83950617284,0.834545547189\n",
      "38,0.834616315662,0.83950617284,0.834545547189\n",
      "39,0.834616315662,0.83950617284,0.834545547189\n",
      "40,0.834616315662,0.83950617284,0.834545547189\n",
      "41,0.834616315662,0.83950617284,0.834545547189\n",
      "42,0.822447003493,0.827160493827,0.822553977043\n",
      "43,0.811720692766,0.814814814815,0.810559656537\n",
      "44,0.783293564339,0.777777777778,0.773200558308\n",
      "45,0.783293564339,0.777777777778,0.773200558308\n",
      "46,0.75904210982,0.753086419753,0.749443570191\n",
      "47,0.772285656397,0.765432098765,0.762856396406\n",
      "48,0.772285656397,0.765432098765,0.762856396406\n",
      "49,0.783293564339,0.777777777778,0.773200558308\n",
      "50,0.783293564339,0.777777777778,0.773200558308\n",
      "51,0.770536362693,0.765432098765,0.759523414196\n",
      "52,0.770536362693,0.765432098765,0.759523414196\n",
      "53,0.770536362693,0.765432098765,0.759523414196\n",
      "54,0.770536362693,0.765432098765,0.759523414196\n",
      "55,0.770536362693,0.765432098765,0.759523414196\n",
      "56,0.75904210982,0.753086419753,0.749443570191\n",
      "57,0.770536362693,0.765432098765,0.759523414196\n",
      "58,0.75904210982,0.753086419753,0.749443570191\n",
      "59,0.753066823437,0.753086419753,0.745199984668\n",
      "60,0.741572570564,0.740740740741,0.735120140663\n",
      "61,0.753066823437,0.753086419753,0.745199984668\n",
      "62,0.770536362693,0.765432098765,0.759523414196\n",
      "63,0.753066823437,0.753086419753,0.745199984668\n",
      "64,0.753066823437,0.753086419753,0.745199984668\n",
      "65,0.77261414854,0.765432098765,0.758774887132\n",
      "66,0.77261414854,0.765432098765,0.758774887132\n",
      "67,0.77261414854,0.765432098765,0.758774887132\n",
      "68,0.77261414854,0.765432098765,0.758774887132\n",
      "69,0.756774652681,0.753086419753,0.74264600843\n",
      "70,0.772026259063,0.765432098765,0.755899340778\n",
      "71,0.772026259063,0.765432098765,0.755899340778\n",
      "72,0.772026259063,0.765432098765,0.755899340778\n",
      "73,0.756774652681,0.753086419753,0.74264600843\n",
      "74,0.756774652681,0.753086419753,0.74264600843\n",
      "75,0.756774652681,0.753086419753,0.74264600843\n",
      "76,0.772026259063,0.765432098765,0.755899340778\n",
      "77,0.756774652681,0.753086419753,0.74264600843\n",
      "78,0.756774652681,0.753086419753,0.74264600843\n",
      "79,0.756774652681,0.753086419753,0.74264600843\n",
      "80,0.756774652681,0.753086419753,0.74264600843\n",
      "81,0.756774652681,0.753086419753,0.74264600843\n",
      "82,0.756774652681,0.753086419753,0.74264600843\n",
      "83,0.756774652681,0.753086419753,0.74264600843\n",
      "84,0.736272007178,0.740740740741,0.729418495202\n",
      "85,0.736272007178,0.740740740741,0.729418495202\n",
      "86,0.736272007178,0.740740740741,0.729418495202\n",
      "87,0.736272007178,0.740740740741,0.729418495202\n",
      "88,0.736272007178,0.740740740741,0.729418495202\n",
      "89,0.756774652681,0.753086419753,0.74264600843\n",
      "90,0.736272007178,0.740740740741,0.729418495202\n",
      "91,0.736272007178,0.740740740741,0.729418495202\n",
      "92,0.721463598852,0.728395061728,0.719356987556\n",
      "93,0.721463598852,0.728395061728,0.719356987556\n",
      "94,0.738781843022,0.740740740741,0.732237111547\n",
      "95,0.738781843022,0.740740740741,0.732237111547\n",
      "96,0.738781843022,0.740740740741,0.732237111547\n",
      "97,0.738781843022,0.740740740741,0.732237111547\n",
      "98,0.738781843022,0.740740740741,0.732237111547\n",
      "99,0.738781843022,0.740740740741,0.732237111547\n"
     ]
    }
   ],
   "source": [
    "for test_k in range(1,100):\n",
    "    Actual = []\n",
    "    Pred = []\n",
    "\n",
    "    for f in fTest:\n",
    "        sumx=0\n",
    "        sumy=0\n",
    "        sumw=0\n",
    "        # What is the test set label?\n",
    "        lab=GTest.node[f][\"label\"]\n",
    "        pred_lab = dict()\n",
    "\n",
    "        for t in GTest.neighbors(f):\n",
    "            # is this test term in our term array?\n",
    "            if t in tArr:\n",
    "                # Get the weight of the edge from the test graph\n",
    "                w=float(GTest.edge[f][t][\"weight\"])\n",
    "                sumw+=w\n",
    "                # Get the x,y of the term from the model graph\n",
    "                sumx+=float(Gr.node[t][\"x\"])*w\n",
    "                sumy+=float(Gr.node[t][\"y\"])*w\n",
    "\n",
    "        if sumw > 0:\n",
    "            # Get the centroid\n",
    "            x = sumx/sumw\n",
    "            y = sumy/sumw\n",
    "            # Query the tree for nearest neighbor\n",
    "            dist,ind = tree.query([[x,y]], k=test_k)\n",
    "            for i in ind[0]:\n",
    "                if Y_label[i] in pred_lab:\n",
    "                    pred_lab[Y_label[i]] += 1\n",
    "                else:\n",
    "                    pred_lab[Y_label[i]] = 1\n",
    "\n",
    "        else:\n",
    "            pred_lab[\"UNK\"] = 1\n",
    "\n",
    "        #print(f + \": \" + lab + \", pred=\" + str(sorted(pred_lab.items(), key=operator.itemgetter(1), reverse=True)[0][0]))\n",
    "        Actual.append(lab)\n",
    "        Pred.append(str(sorted(pred_lab.items(), key=operator.itemgetter(1), reverse=True)[0][0]))\n",
    "\n",
    "    precision = precision_score(Actual, Pred, average='weighted') \n",
    "    recall = recall_score(Actual, Pred, average='weighted')\n",
    "    f1 = f1_score(Actual, Pred, average='weighted') \n",
    "    print(str(test_k) + ',' + str(precision) + ',' + str(recall) + ',' + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cm = confusion_matrix(Actual, Pred)\n",
    "#cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### CLUSTERING\n",
    "\n",
    "\n",
    "# Change text label to number\n",
    "def strToNum(str):\n",
    "    rVal = 0\n",
    "    if str == \"B\":\n",
    "        rVal = 0\n",
    "    elif str == \"E\":\n",
    "        rVal = 1\n",
    "    elif str == \"P\":\n",
    "        rVal = 2\n",
    "    elif str == \"S\":\n",
    "        rVal = 3\n",
    "    elif str == \"T\":\n",
    "        rVal = 4\n",
    "        \n",
    "    return rVal\n",
    "\n",
    "\n",
    "# Load the data from the graph into a dataframe\n",
    "df = pd.DataFrame(columns=['filename','x','y','Label'])\n",
    "df.Label = df.Label.astype(int)\n",
    "\n",
    "for f in fArr:\n",
    "    s = strToNum(f[32:33])\n",
    "    df = df.append({\"filename\": f, \"x\":Gr.node[f][\"x\"], \"y\":Gr.node[f][\"y\"], \"Label\": s}, ignore_index=True)\n",
    "\n",
    "#df = df.append({\"filename\": \"file1\", \"x\":1, \"y\":2}, ignore_index=True)\n",
    "#df = df.append({\"filename\": \"file2\", \"x\":2, \"y\":3}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster the documents using kmeans\n",
    "kmeans = KMeans(n_clusters=5,init='k-means++')\n",
    "result = kmeans.fit_predict(df[[\"x\",\"y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"Cluster\"]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm = confusion_matrix(df.Label, df.Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[213,   0,   0,   2, 295],\n",
       "       [ 79,   2,   0, 293,  12],\n",
       "       [402,   0,   1,   4,  10],\n",
       "       [ 47,   1, 457,   6,   0],\n",
       "       [ 81,   0,   0,  12, 308]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tc = pd.DataFrame(columns=['term','weight','Cluster'])\n",
    "tc.Cluster = tc.Cluster.astype(int)\n",
    "\n",
    "for f in fArr:\n",
    "    dftemp=df[df['filename'] == f]\n",
    "    c = dftemp.get_value(dftemp.index[0], col='Cluster')\n",
    "    for t in Gr.neighbors(f):\n",
    "        w=float(Gr.edge[f][t][\"weight\"])\n",
    "        tc = tc.append({\"term\": t, \"weight\": w, \"Cluster\": c}, ignore_index=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tc.to_csv(\"c:\\\\BBC_Data\\\\tc_out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
