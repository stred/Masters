{"cells":[{"cell_type":"code","source":["print(sc.version)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["### Settings\n\n# Name to use in extracted files\n_set_name = 'BBCN_02'\n\n# in_degree or degree\n_in_degree = True\n\n# Window size to use for graph-of-words extraction\n_windowSize = 3\n\n# Path to source files\n_path_to_files = '/FileStore/data/bbc_news'\n_path_to_test_files = '/FileStore/data/bbc_news_test'\n\n# Path to outputs\n_path_to_output = \"/FileStore/data/bbc_output\"\n\n# If the degree is > windowSize-1 then add the term and edge\n# We use windowSize for in_degree counts, and windowSize*2 for degree counts\n_degree_compare = (_windowSize-1)*2\nif _in_degree == True:\n    _degree_compare = _windowSize-1\n\n    \n# Normalised word limit - value below which connected terms will be removed\n_normalised_term_limit = 0.0\n\n# Limits for connected term removal\n_upper_percentile = 99.7\n_lower_count = 2\n\nprint(\"Done\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Import required libraries\n#from graphframes import *\n\n# NetworkX graph library\nimport networkx as nx\n\n# Random library\nimport random\n\n\nimport re\nimport nltk\n#from nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\nstemmer = PorterStemmer()\n\n# Time Library\nfrom time import time\n\n# Maths library\nimport math\n\n# Scipy stats functions\nimport scipy.stats\nimport numpy as np\n\n# KD Tree algo\nfrom sklearn.neighbors import KDTree\n\n# kmeans cluster algorithm \nfrom sklearn.cluster import KMeans\n\n# Confusion matrix, precision, recall, F1\nfrom sklearn.metrics import *\n\n# Pandas library\nimport pandas as pd\nfrom pandas_ml import ConfusionMatrix\n\n# Operator\nimport operator\n\n# File libraries\nimport os\nimport glob2\nfrom pathlib import Path\nimport csv\n\n# Persistent storage\nfrom pyspark import StorageLevel\n\n# cPickle library\nimport cPickle as pickle\n\n# Setup DataFrames to receive nodes (vertices) and edges\nfrom pyspark.sql.types import *\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# This is a combination of the English stopwords from the NTLK library\n# combined with a list of Google stopwords from https://www.link-assistant.com/seo-stop-words.html\n\ncachedStopWords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'better', 'pleas', 'ye', \"daren't\", 'how', 'upward', 'for', 'could', 'yourselv', 'whenev', 'then', 'whereupon', 'so', 'anyth', 'as', 'themselv', 'thi', 'brief', 'undo', 'onli', 'backward', 'hi', 'six', 'by', 'k', 'quit', 'yourself', 'fairli', 'right', 'fifth', 'wish', 'somebodi', 'did', \"they'v\", 'whose', 'otherwis', 'inward', 'appear', \"who'll\", 'late', 'presum', 'reason', 'chang', 'her', 'outsid', 'clearli', 'sorri', 'new', 'accord', 'not', 'got', 'took', 'provid', 'each', 'selv', 'whether', 'plu', 'serious', 'inner', 'normal', 'me', 'gotten', 'noth', 'mr', 'nevertheless', 'co.', 'abroad', 'unfortun', 'rather', 'particularli', 'former', 'usual', 'away', 'up', 'abov', 'becaus', 'than', 'except', 'inc.', 'can', 'howev', 'via', 'someth', 'along', 'fewer', 'while', 'directli', 'our', 'alon', 'neverless', 'c', 'either', 'must', \"c'mon\", 'differ', 'whilst', 'hundr', 'between', \"mightn't\", 'ever', 'z', 'apart', 'gone', 'low', 'came', 'let', 'like', 'in', 'u', 'asid', \"needn't\", 'again', 'therebi', \"she'll\", 'mainli', 'taken', 'itself', 'thank', 'need', 'sometim', 'until', 'she', 'no', 'say', \"they'll\", 'sensibl', 'us', 'place', \"he'\", 'amongst', \"they'r\", 'hither', \"i'll\", 'indic', 'thing', 'were', 'littl', 'whole', 'nine', 'thanx', 'under', 'an', 'versu', 'therein', 'go', 'appropri', 'would', 'exampl', 'entir', 'beyond', 'near', 'etc', \"doesn't\", 'sure', 's', 'veri', \"shan't\", 'someon', 'dure', 'elsewher', 'myself', 'therefor', 'they', 'about', \"hasn't\", 'ie', 'whichev', 'far', \"i'd\", 'avail', 'possibl', 'current', 'though', 'nd', 'thru', \"isn't\", 'a', 'furthermor', \"haven't\", 'nonetheless', \"hadn't\", 'never', 'cant', 'saw', 'viz', 'their', 'even', \"what'll\", \"can't\", 'am', 'onc', 'amidst', 'that', 'last', 'to', 'd', 'everi', \"couldn't\", 'upon', 'one', 'everywher', 'inde', 'round', 'more', 'amid', 'ha', 'hello', 'certainli', 'wa', 'himself', 'nobodi', 'regardless', 'use', 'well', 'notwithstand', \"shouldn't\", 'same', 'b', 'obvious', \"there'r\", 'j', 'anoth', 'nearli', 'self', 'these', 'mere', 'thu', \"wasn't\", 'meantim', 'with', 'but', 'abl', 'secondli', 'went', \"a'\", \"what'\", 'thereupon', 'thorough', 'appreci', 'dare', 'third', 'tend', 'befor', 'miss', \"we'll\", 'some', \"oughtn't\", \"she'\", \"where'\", 'perhap', 'seen', 'caus', \"what'v\", 'describ', 'think', 'underneath', 'lower', 'twice', 'th', 'few', 'associ', 'insofar', 'valu', 'anybodi', 'latterli', 'moreov', 'co', 'him', 'neither', 'okay', 'mayb', 'do', 'probabl', \"there'd\", 'rel', 'consequ', 'ask', 'howbeit', 'o', 'wherein', 'someday', 'next', 'noon', \"we'v\", 'mani', 'against', 'edu', 'throughout', 'actual', 'et', 'is', 'minu', 'unless', 'yet', \"i'v\", 'eight', 'still', 'name', 'inc', 'mine', 'sup', 'believ', 'somehow', 'you', 'eg', 'whi', 'been', 'where', 'get', 'three', 'g', 'seven', 'everyon', 'made', 'may', 'ani', \"mayn't\", 'recent', 'none', 'be', 'further', 'help', 'toward', \"you'll\", 'consid', 'my', 'meanwhil', 'p', 'what', \"you'r\", \"there'l\", 'els', 'beforehand', 'there', 'becam', 'forth', 'thenc', 'way', 'exactli', 'four', 'hope', 'kept', 'necessari', 'sub', 'forev', 'unlik', 'among', 'at', 'q', 'ex', 'besid', 'such', 't', \"she'd\", 'overal', 'wherebi', 'goe', 'no-on', 'alreadi', 'r', 'nowher', 'togeth', 'whomev', 'keep', \"wouldn't\", 'tell', 'y', 'variou', 'onto', 'given', 'farther', 'sinc', 'later', 'had', 'afterward', \"mustn't\", 'or', 'ourselv', 'also', 'soon', 'see', \"one'\", 'best', 'somewher', \"aren't\", 'less', 'whither', 'lest', 'much', 'n', 'whereaft', 'should', 'although', 'around', \"c'\", 'seem', 'thereaft', \"there'\", 'concern', 'tri', 'que', 'becom', 'accordingli', 'l', 'greet', 'of', 'oh', 'down', 'five', 'seriou', 'them', \"who'\", 'allow', 'on', 'everybodi', \"you'd\", 'truli', 'wherev', 'regard', 'come', 'somewhat', 'ago', 'hereaft', 'shall', \"weren't\", 'e', \"it'll\", 'through', 'wherea', 'most', 'cannot', \"we'r\", 'begin', 'here', \"don't\", 'adj', 'per', 'v', 'whoever', 'out', 'we', \"you'v\", 'opposit', 'second', 'qv', 'past', 'want', 'both', 'likewis', \"there'v\", 're', 'h', 'especi', 'formerli', 'm', 'old', 'anyhow', 'after', 'back', 'definit', 'despit', 'thoroughli', 'anyon', 'done', 'often', 'know', 'w', \"he'll\", 'insid', \"they'd\", \"ain't\", 'latter', 'might', 'evermor', 'now', 'doe', 'caption', 'end', 'take', 'unto', 'certain', 'till', 'neverf', \"here'\", \"won't\", 'aw', 'eighti', 'own', 'correspond', 'henc', 'non', 'ahead', 'sent', 'all', 'ok', 'make', 'which', 'ignor', 'nor', 'give', 'welcom', 'other', 'almost', 'hereupon', 'who', 'wonder', \"i'm\", 'zero', 'specifi', 'rd', 'com', 'everyth', 'said', 'too', 'x', \"didn't\", 'immedi', 'he', 'i', 'realli', 'half', \"it'd\", 'particular', 'herebi', 'vs', 'inasmuch', 'those', 'the', \"t'\", 'contain', 'it', 'first', 'forward', 'herein', 'follow', 'instead', 'just', \"that'v\", 'over', 'have', 'and', \"we'd\", 'nineti', 'when', 'below', \"that'\", 'f', 'if', 'happen', 'behind', 'whom', 'are', 'alongsid', 'will', 'from', 'into', 'enough', 'whatev', 'least', 'your', 'anyway', 'respect', 'found', 'sever', 'un', 'herself', \"let'\", 'ought', 'within', \"it'\", 'novel', 'thirti', 'known', 'look', 'off', 'cours', \"that'll\", 'across', 'whenc', 'anywher', 'downward', 'alway', 'mean', 'two', 'without', 'hardli', \"who'd\", \"he'd\", 'ltd', 'mostli']"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Functions used in the model build\n\n\n# VB style text parsing functions\ndef left(s, amount):\n    return s[:amount]\ndef right(s, amount):\n    return s[-amount:]\ndef mid(s, offset, amount):\n    return s[offset:offset+amount]\n\n### CleanWord\n# This function cleans a piece of text of non letter/space characters\np1 = re.compile(r'[^a-z ]', re.UNICODE)\np2 = re.compile(r' +', re.UNICODE)\ndef CleanWord(w):\n    x = w.lower()\n    #x = re.sub(r'[^a-z ]','',x)\n    x = p1.sub(' ', x) #.strip() \n    x = p2.sub(' ', x).strip()\n    #x = x.split(' ')\n    return x\n\n### StoreGraphToFile\n# This function takes a graph and stores it to file\ndef StoreGraphToFile(Graph, Desc):\n    filename = _path_to_output + '/' + Desc + '.nodes'\n    vertices = sqlContext.createDataFrame(Graph.nodes(data=True), [\"id\", \"data\"])\n    vertices.repartition(1).write.json(filename)\n    StoreEdgesToFile(Graph, Desc)\n    \ndef StoreEdgesToFile(Graph, Desc):\n    filename = _path_to_output + '/' + Desc + '.edges'\n    edges = sqlContext.createDataFrame(Gr.edges(data=True), [\"src\", \"dst\", \"data\"])\n    edges.repartition(1).write.json(filename)\n\n\n### FileToGraph\n# Function to parse out words and bigrams from a text file\ndef FileToGraph(fileTuple):\n    # Array to return from function\n    rArr = []\n\n    # Take the filename from the tuple\n    filename = fileTuple[0]\n    \n    # remove non letters from the text in the tuple\n    r = CleanWord(fileTuple[1])\n    \n    # split into an array, stem, and remove words in the stopword list\n    arr = [stemmer.stem(y) for y in r.split(\" \") if len(y)>2]\n    arr = [word for word in arr if word not in cachedStopWords]\n    \n    # Create a new directed graph\n    G = nx.DiGraph()\n\n    arrLen = len(arr)\n\n    if arrLen >= _windowSize-1:\n        \n        # Add the first few nodes\n        for i in range(0, _windowSize-1):\n            G.add_node(arr[i])\n\n        for i in range(_windowSize-1, arrLen):\n            G.add_node(arr[i])\n            for j in range(0, (_windowSize-1)):\n                src = arr[i-(_windowSize-1)]\n                tgt = arr[i-j]\n                # Only add edge if src is not equal to target\n                if src!=tgt:\n                    if G.has_edge(src, tgt):\n                        # we added this one before, just increase the weight by one\n                        G[src][tgt]['weight'] += 1\n                    else:\n                        # new edge. add with weight=1\n                        G.add_edge(src, tgt, weight=1)\n\n        for i in range(arrLen-(_windowSize-1), arrLen):\n            for j in range(1, (arrLen-i)):\n                src = arr[i]\n                tgt = arr[i+j]\n                # Only add edge if src is not equal to target\n                if src!=tgt:\n                    if G.has_edge(src, tgt):\n                        # we added this one before, just increase the weight by one\n                        G[src][tgt]['weight'] += 1\n                    else:\n                        # new edge. add with weight=1\n                        G.add_edge(src, tgt, weight=1)\n\n        # Get the \"indegree\" or \"degree\" of the terms\n        if _in_degree == True:\n            d = G.in_degree()\n        else:\n            d = G.degree()\n\n        dMax = d[max(d, key=d.get)]\n\n        # Now add all the words the the return array with the filename and the count\n        for term in d:\n            if(dMax>0):\n                normD = float(d[term])/float(dMax)\n            else:\n                normD = 0\n                \n            if normD > _normalised_term_limit and int(d[term])>_degree_compare:    \n                rArr.append(filename + \"\\t\" + term + \"\\t\" + str(normD))\n        \n    # Return the array of graphs and the array of words\n    return rArr\n\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#### TEST - Test the FileToGraph function on fixed text\ntext = 'in natural language processing (NLP) a text graph is a graph representation of a text item '\ntext += '(document, passage or sentence) it is typically created as a preprocessing step to support '\ntext += 'NLP tasks such as text condensation term disambiguation (topic based) text summarization '\ntext += '(summarize large text collections) and relation extraction (extract relations from unstructured text)'\nd = [r'name.ext', text]\n\nprint(\"Testing: \" + str(_degree_compare) + \" \" + str(_normalised_term_limit))\nx = FileToGraph(d)\nfor y in x: \n    print(y)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#################  TEST TEST TEST  \n\nfilename = _path_to_files + '/sport/005.txt'\n#text = Path(filename).read_text()\ntext_files = sc.wholeTextFiles(filename) \\\n    .flatMap(lambda fileTuple: FileToGraph(fileTuple))\n\nfor x in text_files.collect():   # FOR SPARK APPEND: .collect():\n\n    # Split out the \n    y = x.split(\"\\t\")\n    filename = y[0]\n    term = y[1]\n    degree = y[2]\n    label = os.path.basename(os.path.dirname(filename))  ## filename[32:33]  ### Need to derive a new function for this\n    \n    print(label + ': ' + filename + ', ' + term + ', ' + degree)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["\n##### Load files #####\n\n# PYSPARK IMPLEMENTATION: \ntext_files = sc.wholeTextFiles(_path_to_files + \"/*/*\") \\\n    .flatMap(lambda fileTuple: FileToGraph(fileTuple))\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["\n##### Create initial graph #####\nGr=nx.Graph()\n\n# fArr will hold the array of filenames\nfArr = []\nfTest = []\n\n# tArr will hold the array of terms added to the graph\ntArr = []\ntTest = []\n\n# tArr1 will hold the array of terms that we have seen at least once\ntArr1 = []\n\nfor x in text_files.collect():   \n\n    # Split out the \n    y = x.split(\"\\t\")\n    filename = y[0]\n    term = y[1]\n    degree = y[2]\n    \n    # The label is derived from the sub-folder name\n    label = os.path.basename(os.path.dirname(filename))  \n    \n    # If the filename is not already added, add it to the graph\n    if not filename in fArr:\n\n        # Create some random spatial positions\n        xR = 0 #random.random()*1000\n        yR = 0 #random.random()*1000\n        \n        # Add the file node to the graph\n        Gr.add_node(filename, type=\"file\", x=xR, y = yR, label = label)\n        \n        # Add the filename to the file array\n        fArr.append(filename)\n        \n    # If we have not seen this term before, add a node to the graph\n    if not term in tArr:\n        # Create some random spatial positions\n        xR2 = 0 #random.random()*1000\n        yR2 = 0 #random.random()*1000\n\n        # Add the term to the graph\n        Gr.add_node(term, type=\"term\", x=xR2, y=yR2)\n        tArr.append(term)\n\n    # Add the edge to the graph\n    Gr.add_edge(filename, term, weight=degree)\n\n        \n# Remove terms with only one connection or > % fractile # of connections\nd = Gr.degree()\n\n# List to allow calculation of percentile\ndegreeList = []\n\nfor t in tArr:\n    degreeList.append(d[t])\n\n# What #degree is > 99.7    \nperc = np.percentile(degreeList, _upper_percentile)\n\nfor t in tArr:\n    n = d[t]\n    # Remove <2 #####or > 99.7 pecentile\n    if n < _lower_count or n > perc:\n        # Remove Edge\n        for f in Gr.neighbors(t):\n            Gr.remove_edge(f,t)\n        # Remove node\n        Gr.remove_node(t)\n        # Remove from tArr list\n        tArr.remove(t)\n\n# Create the label arrays\nlArr = []\nlDict = dict()\n\nfor f in fArr:\n    # Get the label\n    label = Gr.node[f][\"label\"]\n    # Add the label to the label array\n    if not label in lArr:\n        lArr.append(label)\n        lDict[label]=1\n    else:\n        lDict[label]+=1\n    \n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["\n# Calculate the initial file cluster position\nclustRad = 1000\n\n# lAngle is the full circle (in radians) divided by number of files\nlAngle = 2*math.pi / len(fArr)\nlastEnd = 0\nlStart = dict()\nlEnd = dict()\n\n# Go through each label and calculate its span as \n# No. of files in class * angle size\nfor l in lArr:\n    print(l)\n    lStart[l] = lastEnd\n    lEnd[l] = lastEnd + (lDict[l]*lAngle)\n    lastEnd = lEnd[l]\n\n# Reset x,y for files based on calculated centres\nfor f in fArr:\n    # Reset x,y for each file\n    # Get the centre from the label\n    c = Gr.node[f][\"label\"]\n\n    # centre of arc\n    cCentre = (lEnd[c]-lStart[c])*.5\n    clustAng = lStart[c] + cCentre\n\n    # Set the x, y for the file\n    Gr.node[f][\"x\"] = clustRad+(math.cos(clustAng)*clustRad)\n    Gr.node[f][\"y\"] = clustRad+(math.sin(clustAng)*clustRad)\n\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["########## Build the Model\n\n# Reposition terms, then files\n\n# Reposition all the term nodes in the centroid of their documents\nfor t in tArr:\n    sumx=0\n    sumy=0\n    sumw=0\n\n    for f in Gr.neighbors(t):\n        w=float(Gr.edge[f][t][\"weight\"])\n        sumw+=w\n        sumx+=float(Gr.node[f][\"x\"])*w\n        sumy+=float(Gr.node[f][\"y\"])*w\n\n    if sumw > 0:\n        Gr.node[t][\"x\"] = sumx/sumw\n        Gr.node[t][\"y\"] = sumy/sumw\n    else:\n        print(\"0 weight on term \" + t)\n\nStoreGraphToFile(Gr, _set_name + \".1.Reposition Terms\")\n\npSumx = 0\npSumy = 0\n\n\n# Reposition all the file nodes in the centroid of their terms\nfor f in fArr:\n    sumx=0\n    sumy=0\n    sumw=0\n\n    for t in Gr.neighbors(f):\n        w=float(Gr.edge[f][t][\"weight\"])\n        sumw+=w\n        sumx+=float(Gr.node[t][\"x\"])*w\n        sumy+=float(Gr.node[t][\"y\"])*w\n\n    if sumw > 0:\n        Gr.node[f][\"x\"] = sumx/sumw\n        pSumx += sumx/sumw\n        Gr.node[f][\"y\"] = sumy/sumw\n        pSumy += sumy/sumw\n\n## Store results for visualisation\nStoreGraphToFile(Gr, _set_name + \".2.Reposition Files\")\n\n# Extract values for Tree from Graph\nTree_Array = [];\nTree_label = [];\n\nfor f in fArr:\n    x = Gr.node[f][\"x\"]\n    y = Gr.node[f][\"y\"]\n    lab = Gr.node[f][\"label\"]\n    Tree_Array.append([x,y])\n    Tree_label.append(lab)\n\ntree = KDTree(Tree_Array)\n\n# Persist the model\npickle.dump(tree,open(\"PredictionTree.pkl\",\"wb\"))\npickle.dump(Gr,open(\"PredictionGraph.pkl\",\"wb\"))\npickle.dump(tArr,open(\"PredictionTermArray.pkl\",\"wb\"))\n\n## Load model from Pickle\n# tree = pickle.load(open(\"PredictionTree.pkl\",\"rb\"))\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["## Create the model graph as a DataFrames (GraphFrame not necessary)\n#vertices = sqlContext.createDataFrame(Gr.nodes(data=True), [\"id\", \"data\"])\n\n# Persist data frames\n#vertices.persist(StorageLevel.MEMORY_AND_DISK)\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["def predict(fileTuple): #text, test_k, single_value_only=True):\n    \n    test_k = 13\n    single_value_only=True\n    \n    sumx=0\n    sumy=0\n    sumw=0\n    pred_lab = dict()\n    \n    label = os.path.basename(os.path.dirname(fileTuple[0]))\n    \n    rArr = FileToGraph(fileTuple)\n\n    # Predict base on returned array\n    for r in rArr:\n        ts = r.split('\\t')\n        t = ts[1]\n        w = float(ts[2])\n        \n        if t in tArr:\n            sumw+=w\n            # Get the x,y of the term from the model graph\n            sumx+=float(Gr.node[t][\"x\"])*w #float(vertices.filter(\"id='\" + t + \"'\").select(\"data\").first()[0].get('x'))*w\n            sumy+=float(Gr.node[t][\"y\"])*w #float(vertices.filter(\"id='\" + t + \"'\").select(\"data\").first()[0].get('y'))*w\n\n    if sumw > 0:\n        # Get the centroid\n        x = sumx/sumw\n        y = sumy/sumw\n        # Query the tree for nearest neighbor\n        dist,ind = tree.query([[x,y]], k=test_k)\n        for i in ind[0]:\n            if Tree_label[i] in pred_lab:\n                pred_lab[Tree_label[i]] += 1\n            else:\n                pred_lab[Tree_label[i]] = 1\n\n    else:\n        pred_lab[\"UNK\"] = 1\n    \n    # Set the return value to be the highest matched label\n    rVal = sorted(pred_lab.items(), key=operator.itemgetter(1), reverse=True)[0][0]\n    if single_value_only==False:\n        # Return the entire result dictionary\n        rVal = pred_lab\n        for l in pred_lab:\n            pred_lab[l] = float(pred_lab[l]) / float(test_k)\n    \n    return [filename + \"\\t\" + label + \"\\t\" + rVal]\n\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#### TEST - Test the FileToGraph function on fixed text\ntext = 'Runs and wickets from Australian paceman James Pattinson '\ntext += 'and England''s Stuart Broad put Nottinghamshire firmly in command at Leicestershire. '\ntext += 'Ben Raine (6-66) had helped reduce the visitors to 167-7, '\ntext += 'still 84 runs adrift of the hosts'' first-innings 251. '\ntext += 'But Broad (52) and Pattinson (89) added 122 for the eighth wicket to help Notts to a 78-run lead. '\ntext += 'Luke Fletcher then piled in with three wickets as the hosts slumped to 51-6 at the close. '\ntext += 'Making his Notts debut, Pattinson hit 14 boundaries and two sixes '\ntext += 'in his 108-ball knock, and then had opener Harry Dearden caught behind '\ntext += 'by Chris Read with the ninth ball of Leicestershire''s second innings. '\ntext += 'Fletcher replaced Broad, who trapped Paul Horton lbw for two, and '\ntext += 'produced an excellent spell of line and length to dismantle '\ntext += 'Leicestershire''s middle order, putting Notts on the brink of victory. '\nd = [r'path/test1/name1.ext', text]\n\nx = predict(d)\nprint(x)\nfor y in x: \n    print(y)\n    \ntext = 'in natural language processing (NLP) a text graph is a graph representation of a text item '\ntext += '(document, passage or sentence) it is typically created as a preprocessing step to support '\ntext += 'NLP tasks such as text condensation term disambiguation (topic based) text summarization '\ntext += '(summarize large text collections) and relation extraction (extract relations from unstructured text)'\nd = [r'path/test2/name2.ext', text]\n\nx = predict(d)\nprint(x)\nfor y in x: \n    print(y)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# PYSPARK IMPLEMENTATION: \npredictions = sc.wholeTextFiles(_path_to_test_files + \"/*/*\")  \\\n    .flatMap(lambda fileTuple: predict(fileTuple))\n  "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["\nActual = []\nPred = []\nLabels = []\n\n# Get all the (files - one file at a time)\nfor x in predictions.collect():\n\n    # Split out the \n    y = x.split(\"\\t\")\n    filename = y[0]\n    label = y[1]\n    pred = y[2]    \n    \n    Actual.append(label)\n    Pred.append(pred)\n\n    if not label in Labels:\n        Labels.append(label)\n    if not pred in Labels:\n        Labels.append(pred)\n    \naccuracy = accuracy_score(Actual, Pred)\nprecision = precision_score(Actual, Pred, average='weighted') \nrecall = recall_score(Actual, Pred, average='weighted')\nf1 = f1_score(Actual, Pred, average='weighted') \n\ncm = ConfusionMatrix(Actual, Pred, Labels)\n  \n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 400)\n\nprint(cm)\nprint(\"\")\nprint(\"Accuracy\\t\" + str(accuracy))\nprint(\"Precision\\t\" + str(precision))\nprint(\"Recall\\t\" + str(recall))\nprint(\"F1\\t\" + str(f1))\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"TextCluster","notebookId":1737300674116084},"nbformat":4,"nbformat_minor":0}
