{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "## Settings\n",
    "#_path_to_settings_files = \"C:\\\\Users\\\\Stephen\\\\OneDrive\\\\Documents\\\\NCI\\\\Thesis\\\\Data\"\n",
    "\n",
    "# Name to use in extracted files\n",
    "_set_name = 'BBCN01'\n",
    "\n",
    "# in_degree or degree\n",
    "_in_degree = True\n",
    "\n",
    "# Window size to use for graph-of-words extraction\n",
    "_windowSize = 3\n",
    "\n",
    "# Path to Train and Test files\n",
    "\n",
    "_path_to_files = 'C:\\\\BBC_Data\\\\bbc-fulltext\\\\TrainTest\\\\Train'\n",
    "_path_to_test_files = 'C:\\\\BBC_Data\\\\bbc-fulltext\\\\TrainTest\\\\Test'\n",
    "\n",
    "# Path to outputs\n",
    "_path_to_output = 'C:\\\\BBC_Data\\\\bbc-fulltext\\\\TrainTest\\\\Output'\n",
    "\n",
    "# If the degree is > windowSize-1 then add the term and edge\n",
    "# We use windowSize for in_degree counts, and windowSize*2 for degree counts\n",
    "_degree_compare = (_windowSize-1)*2\n",
    "if _in_degree == True:\n",
    "    _degree_compare = _windowSize-1\n",
    "\n",
    "    \n",
    "# Normalised word limit - value below which connected terms will be removed\n",
    "_normalised_term_limit = 0.0\n",
    "\n",
    "# Limits for connected term removal\n",
    "_upper_percentile = 99.7\n",
    "_lower_count = 2\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "###### Important - nltk 3.2.2 has a bug which breaks on some words (e.g. oed)\n",
    "###### https://github.com/nltk/nltk/issues/1581\n",
    "###### Recommend v3.2.1 until 3.2.3+ is available\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# NetworkX graph library\n",
    "import networkx as nx\n",
    "\n",
    "# Random library\n",
    "import random\n",
    "\n",
    "# File libraries\n",
    "import os\n",
    "import glob2\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# Time Library\n",
    "from time import time\n",
    "\n",
    "# Maths library\n",
    "import math\n",
    "\n",
    "# Scipy stats functions\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "# KD Tree algo\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# kmeans cluster algorithm \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Confusion matrix, precision, recall, F1\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Pandas library\n",
    "import pandas as pd\n",
    "from pandas_ml import ConfusionMatrix\n",
    "\n",
    "# Operator\n",
    "import operator\n",
    "\n",
    "# Pickle Library\n",
    "import pickle\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Stop Words List\n",
    "## Get list of Google stopwords from file\n",
    "#fname = _path_to_settings_files + \"\\\\GoogleStopwords.txt\"\n",
    "#with open(fname) as f:\n",
    "#    StopWords = f.readlines()\n",
    "## remove whitespace characters like `\\n` at the end of each line\n",
    "#StopWords = [x.strip() for x in StopWords]\n",
    "## Stem the stopwords\n",
    "#StopWords = list(set([stemmer.stem(y) for y in StopWords]))\n",
    "## Create list of stop words to use- first the NLTK stopwords\n",
    "#cachedStopWords = stopwords.words(\"english\")\n",
    "## Add in the Google Stopwords\n",
    "#cachedStopWords += StopWords\n",
    "##print(cachedStopWords)\n",
    "\n",
    "# This is a combination of the English stopwords from the NTLK library\n",
    "# combined with a list of Google stopwords from https://www.link-assistant.com/seo-stop-words.html\n",
    "\n",
    "cachedStopWords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'better', 'pleas', 'ye', \"daren't\", 'how', 'upward', 'for', 'could', 'yourselv', 'whenev', 'then', 'whereupon', 'so', 'anyth', 'as', 'themselv', 'thi', 'brief', 'undo', 'onli', 'backward', 'hi', 'six', 'by', 'k', 'quit', 'yourself', 'fairli', 'right', 'fifth', 'wish', 'somebodi', 'did', \"they'v\", 'whose', 'otherwis', 'inward', 'appear', \"who'll\", 'late', 'presum', 'reason', 'chang', 'her', 'outsid', 'clearli', 'sorri', 'new', 'accord', 'not', 'got', 'took', 'provid', 'each', 'selv', 'whether', 'plu', 'serious', 'inner', 'normal', 'me', 'gotten', 'noth', 'mr', 'nevertheless', 'co.', 'abroad', 'unfortun', 'rather', 'particularli', 'former', 'usual', 'away', 'up', 'abov', 'becaus', 'than', 'except', 'inc.', 'can', 'howev', 'via', 'someth', 'along', 'fewer', 'while', 'directli', 'our', 'alon', 'neverless', 'c', 'either', 'must', \"c'mon\", 'differ', 'whilst', 'hundr', 'between', \"mightn't\", 'ever', 'z', 'apart', 'gone', 'low', 'came', 'let', 'like', 'in', 'u', 'asid', \"needn't\", 'again', 'therebi', \"she'll\", 'mainli', 'taken', 'itself', 'thank', 'need', 'sometim', 'until', 'she', 'no', 'say', \"they'll\", 'sensibl', 'us', 'place', \"he'\", 'amongst', \"they'r\", 'hither', \"i'll\", 'indic', 'thing', 'were', 'littl', 'whole', 'nine', 'thanx', 'under', 'an', 'versu', 'therein', 'go', 'appropri', 'would', 'exampl', 'entir', 'beyond', 'near', 'etc', \"doesn't\", 'sure', 's', 'veri', \"shan't\", 'someon', 'dure', 'elsewher', 'myself', 'therefor', 'they', 'about', \"hasn't\", 'ie', 'whichev', 'far', \"i'd\", 'avail', 'possibl', 'current', 'though', 'nd', 'thru', \"isn't\", 'a', 'furthermor', \"haven't\", 'nonetheless', \"hadn't\", 'never', 'cant', 'saw', 'viz', 'their', 'even', \"what'll\", \"can't\", 'am', 'onc', 'amidst', 'that', 'last', 'to', 'd', 'everi', \"couldn't\", 'upon', 'one', 'everywher', 'inde', 'round', 'more', 'amid', 'ha', 'hello', 'certainli', 'wa', 'himself', 'nobodi', 'regardless', 'use', 'well', 'notwithstand', \"shouldn't\", 'same', 'b', 'obvious', \"there'r\", 'j', 'anoth', 'nearli', 'self', 'these', 'mere', 'thu', \"wasn't\", 'meantim', 'with', 'but', 'abl', 'secondli', 'went', \"a'\", \"what'\", 'thereupon', 'thorough', 'appreci', 'dare', 'third', 'tend', 'befor', 'miss', \"we'll\", 'some', \"oughtn't\", \"she'\", \"where'\", 'perhap', 'seen', 'caus', \"what'v\", 'describ', 'think', 'underneath', 'lower', 'twice', 'th', 'few', 'associ', 'insofar', 'valu', 'anybodi', 'latterli', 'moreov', 'co', 'him', 'neither', 'okay', 'mayb', 'do', 'probabl', \"there'd\", 'rel', 'consequ', 'ask', 'howbeit', 'o', 'wherein', 'someday', 'next', 'noon', \"we'v\", 'mani', 'against', 'edu', 'throughout', 'actual', 'et', 'is', 'minu', 'unless', 'yet', \"i'v\", 'eight', 'still', 'name', 'inc', 'mine', 'sup', 'believ', 'somehow', 'you', 'eg', 'whi', 'been', 'where', 'get', 'three', 'g', 'seven', 'everyon', 'made', 'may', 'ani', \"mayn't\", 'recent', 'none', 'be', 'further', 'help', 'toward', \"you'll\", 'consid', 'my', 'meanwhil', 'p', 'what', \"you'r\", \"there'l\", 'els', 'beforehand', 'there', 'becam', 'forth', 'thenc', 'way', 'exactli', 'four', 'hope', 'kept', 'necessari', 'sub', 'forev', 'unlik', 'among', 'at', 'q', 'ex', 'besid', 'such', 't', \"she'd\", 'overal', 'wherebi', 'goe', 'no-on', 'alreadi', 'r', 'nowher', 'togeth', 'whomev', 'keep', \"wouldn't\", 'tell', 'y', 'variou', 'onto', 'given', 'farther', 'sinc', 'later', 'had', 'afterward', \"mustn't\", 'or', 'ourselv', 'also', 'soon', 'see', \"one'\", 'best', 'somewher', \"aren't\", 'less', 'whither', 'lest', 'much', 'n', 'whereaft', 'should', 'although', 'around', \"c'\", 'seem', 'thereaft', \"there'\", 'concern', 'tri', 'que', 'becom', 'accordingli', 'l', 'greet', 'of', 'oh', 'down', 'five', 'seriou', 'them', \"who'\", 'allow', 'on', 'everybodi', \"you'd\", 'truli', 'wherev', 'regard', 'come', 'somewhat', 'ago', 'hereaft', 'shall', \"weren't\", 'e', \"it'll\", 'through', 'wherea', 'most', 'cannot', \"we'r\", 'begin', 'here', \"don't\", 'adj', 'per', 'v', 'whoever', 'out', 'we', \"you'v\", 'opposit', 'second', 'qv', 'past', 'want', 'both', 'likewis', \"there'v\", 're', 'h', 'especi', 'formerli', 'm', 'old', 'anyhow', 'after', 'back', 'definit', 'despit', 'thoroughli', 'anyon', 'done', 'often', 'know', 'w', \"he'll\", 'insid', \"they'd\", \"ain't\", 'latter', 'might', 'evermor', 'now', 'doe', 'caption', 'end', 'take', 'unto', 'certain', 'till', 'neverf', \"here'\", \"won't\", 'aw', 'eighti', 'own', 'correspond', 'henc', 'non', 'ahead', 'sent', 'all', 'ok', 'make', 'which', 'ignor', 'nor', 'give', 'welcom', 'other', 'almost', 'hereupon', 'who', 'wonder', \"i'm\", 'zero', 'specifi', 'rd', 'com', 'everyth', 'said', 'too', 'x', \"didn't\", 'immedi', 'he', 'i', 'realli', 'half', \"it'd\", 'particular', 'herebi', 'vs', 'inasmuch', 'those', 'the', \"t'\", 'contain', 'it', 'first', 'forward', 'herein', 'follow', 'instead', 'just', \"that'v\", 'over', 'have', 'and', \"we'd\", 'nineti', 'when', 'below', \"that'\", 'f', 'if', 'happen', 'behind', 'whom', 'are', 'alongsid', 'will', 'from', 'into', 'enough', 'whatev', 'least', 'your', 'anyway', 'respect', 'found', 'sever', 'un', 'herself', \"let'\", 'ought', 'within', \"it'\", 'novel', 'thirti', 'known', 'look', 'off', 'cours', \"that'll\", 'across', 'whenc', 'anywher', 'downward', 'alway', 'mean', 'two', 'without', 'hardli', \"who'd\", \"he'd\", 'ltd', 'mostli']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Functions used in the model build\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VB style text parsing functions\n",
    "def left(s, amount):\n",
    "    return s[:amount]\n",
    "def right(s, amount):\n",
    "    return s[-amount:]\n",
    "def mid(s, offset, amount):\n",
    "    return s[offset:offset+amount]\n",
    "\n",
    "### CleanWord\n",
    "# This function cleans a piece of text of non letter/space characters\n",
    "p1 = re.compile(r'[^a-z ]', re.UNICODE)\n",
    "p2 = re.compile(r' +', re.UNICODE)\n",
    "def CleanWord(w):\n",
    "    x = w.lower()\n",
    "    #x = re.sub(r'[^a-z ]','',x)\n",
    "    x = p1.sub(' ', x) #.strip() \n",
    "    x = p2.sub(' ', x).strip()\n",
    "    #x = x.split(' ')\n",
    "    return x\n",
    "\n",
    "### StoreGraphToFile\n",
    "# This function takes a graph and stores it to file\n",
    "def StoreGraphToFile(Graph, Desc):\n",
    "    filename = _path_to_output + '\\\\' + Desc + '.csv'\n",
    "    myfile = open(filename, 'w')\n",
    "    #wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    for n,d in Graph.nodes_iter(data=True):\n",
    "        mylist=str(n) + \"\\t\" + str(d[\"type\"]) + \"\\t\" + str(d[\"x\"]) + \"\\t\" + str(d[\"y\"]) + \"\\n\"\n",
    "        myfile.write(mylist)\n",
    "\n",
    "    myfile.close()\n",
    "    \n",
    "    StoreEdgesToFile(Graph, \"e\" + Desc + \"_edges\")\n",
    "\n",
    "def StoreEdgesToFile(Graph, Desc):\n",
    "    filename = _path_to_output + '\\\\' + Desc + '.csv'\n",
    "\n",
    "    myfile = open(filename, 'w')\n",
    "\n",
    "    for e in Graph.edges_iter():\n",
    "        mylist=e[0] + \"\\t\" + e[1] + \"\\t\" + str(Graph.edge[e[0]][e[1]][\"weight\"]) + \"\\n\"\n",
    "        myfile.write(mylist)\n",
    "\n",
    "    myfile.close()\n",
    "\n",
    "\n",
    "### FileToGraph\n",
    "# Function to parse out words and bigrams from a text file\n",
    "def FileToGraph(fileTuple):\n",
    "    # Array to return from function\n",
    "    rArr = []\n",
    "\n",
    "    # Take the filename from the tuple\n",
    "    filename = fileTuple[0]\n",
    "    \n",
    "    # remove non letters from the text in the tuple\n",
    "    r = CleanWord(fileTuple[1])\n",
    "    \n",
    "    # split into an array, stem, and remove words in the stopword list\n",
    "    arr = [stemmer.stem(y) for y in r.split(\" \") if len(y)>2]\n",
    "    arr = [word for word in arr if word not in cachedStopWords]\n",
    "    \n",
    "    # Create a new directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    arrLen = len(arr)\n",
    "\n",
    "    if arrLen >= _windowSize-1:\n",
    "        \n",
    "        # Add the first few nodes\n",
    "        for i in range(0, _windowSize-1):\n",
    "            G.add_node(arr[i])\n",
    "\n",
    "        for i in range(_windowSize-1, arrLen):\n",
    "            G.add_node(arr[i])\n",
    "            for j in range(0, (_windowSize-1)):\n",
    "                src = arr[i-(_windowSize-1)]\n",
    "                tgt = arr[i-j]\n",
    "                # Only add edge if src is not equal to target\n",
    "                if src!=tgt:\n",
    "                    if G.has_edge(src, tgt):\n",
    "                        # we added this one before, just increase the weight by one\n",
    "                        G[src][tgt]['weight'] += 1\n",
    "                    else:\n",
    "                        # new edge. add with weight=1\n",
    "                        G.add_edge(src, tgt, weight=1)\n",
    "\n",
    "        for i in range(arrLen-(_windowSize-1), arrLen):\n",
    "            for j in range(1, (arrLen-i)):\n",
    "                src = arr[i]\n",
    "                tgt = arr[i+j]\n",
    "                # Only add edge if src is not equal to target\n",
    "                if src!=tgt:\n",
    "                    if G.has_edge(src, tgt):\n",
    "                        # we added this one before, just increase the weight by one\n",
    "                        G[src][tgt]['weight'] += 1\n",
    "                    else:\n",
    "                        # new edge. add with weight=1\n",
    "                        G.add_edge(src, tgt, weight=1)\n",
    "\n",
    "        # Get the \"indegree\" or \"degree\" of the terms\n",
    "        if _in_degree == True:\n",
    "            d = G.in_degree()\n",
    "        else:\n",
    "            d = G.degree()\n",
    "\n",
    "        dMax = d[max(d, key=d.get)]\n",
    "\n",
    "        # Now add all the words the the return array with the filename and the count\n",
    "        for term in d:\n",
    "            if(dMax>0):\n",
    "                normD = float(d[term])/float(dMax)\n",
    "            else:\n",
    "                normD = 0\n",
    "                \n",
    "            if normD > _normalised_term_limit and int(d[term])>_degree_compare:    \n",
    "                rArr.append(filename + \"\\t\" + term + \"\\t\" + str(normD))\n",
    "        \n",
    "    # Return the array of graphs and the array of words\n",
    "    return rArr\n",
    "\n",
    "\n",
    "### GraphTextFiles\n",
    "# Get a list of all the files from the source\n",
    "# LOCAL PYTHON IMPLEMENTATION:\n",
    "def GraphTextFiles(p, ext):\n",
    "\n",
    "    # If this is a folder, we need to append ** for glob\n",
    "    if(os.path.isdir(p)==True):\n",
    "        p+=\"\\\\**\"\n",
    "    \n",
    "    f_list = glob2.glob(p)\n",
    "    rVal = []\n",
    "    testSet = []\n",
    "    \n",
    "    # Get all the (files - one file at a time)\n",
    "    for f in f_list:\n",
    "        bOK = True\n",
    "        if len(ext) > 0:\n",
    "            if f.endswith(ext):\n",
    "                bOK = True\n",
    "            else:\n",
    "                bOK = False\n",
    "        \n",
    "        if bOK==True and os.path.isfile(f)==True:\n",
    "            contents = Path(f).read_text()\n",
    "            rVal += FileToGraph((f, contents))\n",
    "        \n",
    "    return rVal \n",
    "\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 2 0.0\n",
      "name.ext\tnlp\t0.36363636363636365\n",
      "name.ext\trelat\t0.2727272727272727\n",
      "name.ext\ttext\t1.0\n"
     ]
    }
   ],
   "source": [
    "#### TEST - Test the FileToGraph function on fixed text\n",
    "text = 'in natural language processing (NLP) a text graph is a graph representation of a text item '\n",
    "text += '(document, passage or sentence) it is typically created as a preprocessing step to support '\n",
    "text += 'NLP tasks such as text condensation term disambiguation (topic based) text summarization '\n",
    "text += '(summarize large text collections) and relation extraction (extract relations from unstructured text)'\n",
    "d = [r'name.ext', text]\n",
    "\n",
    "print(\"Testing: \" + str(_degree_compare) + \" \" + str(_normalised_term_limit))\n",
    "x = FileToGraph(d)\n",
    "for y in x: \n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name.ext\trecord\t0.8333333333333334\n",
      "name.ext\tindoor\t0.6666666666666666\n",
      "name.ext\tlap\t0.6666666666666666\n",
      "name.ext\tcragg\t0.6666666666666666\n",
      "name.ext\tdibaba\t0.6666666666666666\n",
      "name.ext\tceplak\t0.6666666666666666\n",
      "name.ext\twomen\t0.6666666666666666\n",
      "name.ext\twin\t0.6666666666666666\n",
      "name.ext\tkluft\t0.6666666666666666\n",
      "name.ext\tset\t0.6666666666666666\n",
      "name.ext\tworld\t0.8333333333333334\n",
      "name.ext\tbekel\t1.0\n"
     ]
    }
   ],
   "source": [
    "#### TEST - Test the FileToGraph function on a file from the traning set\n",
    "filename = _path_to_files + '\\\\Sport\\\\005.txt'\n",
    "text = Path(filename).read_text()\n",
    "d = [r'name.ext', text]\n",
    "x = FileToGraph(d)\n",
    "for y in x: \n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 48.279s.\n"
     ]
    }
   ],
   "source": [
    "##### Load files #####\n",
    "# This loads all files from the training folders and parses them to graph triples\n",
    "t0 = time()\n",
    "\n",
    "text_files = GraphTextFiles(_path_to_files, \"\")\n",
    "\n",
    "# PYSPARK IMPLEMENTATION: \n",
    "#text_files = sc.wholeTextFiles(\"C:\\\\BBC_Data\\\\bbc-fulltext\\\\bbcAll\\\\*.txt\") \\\n",
    "#    .flatMap(lambda fileTuple: FileToGraph(fileTuple))\n",
    "#text_files.saveAsTextFile(\"C:\\\\BBC_Data\\\\Output\\\\files\")\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 13.914s.\n"
     ]
    }
   ],
   "source": [
    "##### Create bipartite graph from triples #####\n",
    "Gr=nx.Graph()\n",
    "\n",
    "# fArr will hold the array of filenames\n",
    "fArr = []\n",
    "\n",
    "# tArr will hold the array of terms added to the graph\n",
    "tArr = []\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "for x in text_files:   # FOR SPARK APPEND: .collect():\n",
    "\n",
    "    # Split out the triples\n",
    "    y = x.split(\"\\t\")\n",
    "    filename = y[0]\n",
    "    term = y[1]\n",
    "    degree = y[2]\n",
    "    \n",
    "    # The label is derived from the sub-folder name\n",
    "    label = os.path.basename(os.path.dirname(filename))  \n",
    "    \n",
    "    # If the filename is not already added, add it to the graph\n",
    "    if not filename in fArr:\n",
    "\n",
    "        # Create some random spatial positions\n",
    "        xR = 0 #random.random()*1000\n",
    "        yR = 0 #random.random()*1000\n",
    "        \n",
    "        # Add the file node to the graph\n",
    "        Gr.add_node(filename, type=\"file\", x=xR, y = yR, label = label)\n",
    "        \n",
    "        # Add the filename to the file array\n",
    "        fArr.append(filename)\n",
    "        \n",
    "    # If we have not seen this term before, add a node to the graph\n",
    "    if not term in tArr:\n",
    "        # Create some random spatial positions\n",
    "        xR2 = 0 #random.random()*1000\n",
    "        yR2 = 0 #random.random()*1000\n",
    "\n",
    "        # Add the term to the graph\n",
    "        Gr.add_node(term, type=\"term\", x=xR2, y=yR2)\n",
    "        tArr.append(term)\n",
    "\n",
    "    # Add the edge to the graph\n",
    "    Gr.add_edge(filename, term, weight=degree)\n",
    "\n",
    "        \n",
    "# Remove terms with only one connection or > % fractile # of connections\n",
    "d = Gr.degree()\n",
    "\n",
    "# List to allow calculation of percentile\n",
    "degreeList = []\n",
    "\n",
    "for t in tArr:\n",
    "    degreeList.append(d[t])\n",
    "\n",
    "# What #degree is > 99.7    \n",
    "perc = np.percentile(degreeList, _upper_percentile)\n",
    "\n",
    "for t in tArr:\n",
    "    n = d[t]\n",
    "    # Remove <2 #####or > 99.7 pecentile\n",
    "    if n < _lower_count or n > perc:\n",
    "        # Remove Edge\n",
    "        for f in Gr.neighbors(t):\n",
    "            Gr.remove_edge(f,t)\n",
    "        # Remove node\n",
    "        Gr.remove_node(t)\n",
    "        # Remove from tArr list\n",
    "        tArr.remove(t)\n",
    "\n",
    "# Create the label arrays\n",
    "lArr = []\n",
    "lDict = dict()\n",
    "\n",
    "for f in fArr:\n",
    "    # Get the label\n",
    "    label = Gr.node[f][\"label\"]\n",
    "    # Add the label to the label array\n",
    "    if not label in lArr:\n",
    "        lArr.append(label)\n",
    "        lDict[label]=1\n",
    "    else:\n",
    "        lDict[label]+=1\n",
    "    \n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business\n",
      "entertainment\n",
      "politics\n",
      "sport\n",
      "tech\n"
     ]
    }
   ],
   "source": [
    "### Map the initial file nodes onto the circumference of a circle\n",
    "# Files are positioned in their label clusters, with an arc gap\n",
    "# based on the number of nodes in the class\n",
    "\n",
    "# Set the radius of the circle\n",
    "clustRad = 1000\n",
    "\n",
    "# lAngle is the full circle (in radians) divided by number of files\n",
    "lAngle = 2*math.pi / len(fArr)\n",
    "lastEnd = 0\n",
    "lStart = dict()\n",
    "lEnd = dict()\n",
    "\n",
    "# Go through each label and calculate its span as \n",
    "# No. of files in class * angle size\n",
    "for l in lArr:\n",
    "    print(l)\n",
    "    lStart[l] = lastEnd\n",
    "    lEnd[l] = lastEnd + (lDict[l]*lAngle)\n",
    "    lastEnd = lEnd[l]\n",
    "\n",
    "# Reset x,y for files based on calculated centres\n",
    "for f in fArr:\n",
    "    # Reset x,y for each file\n",
    "    # Get the centre from the label\n",
    "    c = Gr.node[f][\"label\"]\n",
    "\n",
    "    # centre of arc\n",
    "    cCentre = (lEnd[c]-lStart[c])*.5\n",
    "    clustAng = lStart[c] + cCentre\n",
    "\n",
    "    # Set the x, y for the file\n",
    "    Gr.node[f][\"x\"] = clustRad+(math.cos(clustAng)*clustRad)\n",
    "    Gr.node[f][\"y\"] = clustRad+(math.sin(clustAng)*clustRad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.391s.\n"
     ]
    }
   ],
   "source": [
    "########## Build the Model\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Reposition terms, then files\n",
    "\n",
    "# Reposition all the term nodes in the weighted centroid of their documents\n",
    "for t in tArr:\n",
    "    sumx=0\n",
    "    sumy=0\n",
    "    sumw=0\n",
    "\n",
    "    for f in Gr.neighbors(t):\n",
    "        w=float(Gr.edge[f][t][\"weight\"])\n",
    "        sumw+=w\n",
    "        sumx+=float(Gr.node[f][\"x\"])*w\n",
    "        sumy+=float(Gr.node[f][\"y\"])*w\n",
    "\n",
    "    if sumw > 0:\n",
    "        Gr.node[t][\"x\"] = sumx/sumw\n",
    "        Gr.node[t][\"y\"] = sumy/sumw\n",
    "    else:\n",
    "        print(\"0 weight on term \" + t)\n",
    "\n",
    "StoreGraphToFile(Gr, _set_name + \".1.Reposition Terms\")\n",
    "\n",
    "pSumx = 0\n",
    "pSumy = 0\n",
    "\n",
    "# Reposition all the file nodes in the centroid of their terms\n",
    "for f in fArr:\n",
    "    sumx=0\n",
    "    sumy=0\n",
    "    sumw=0\n",
    "\n",
    "    for t in Gr.neighbors(f):\n",
    "        w=float(Gr.edge[f][t][\"weight\"])\n",
    "        sumw+=w\n",
    "        sumx+=float(Gr.node[t][\"x\"])*w\n",
    "        sumy+=float(Gr.node[t][\"y\"])*w\n",
    "\n",
    "    if sumw > 0:\n",
    "        Gr.node[f][\"x\"] = sumx/sumw\n",
    "        pSumx += sumx/sumw\n",
    "        Gr.node[f][\"y\"] = sumy/sumw\n",
    "        pSumy += sumy/sumw\n",
    "\n",
    "# Store results for visualisation\n",
    "StoreGraphToFile(Gr, _set_name + \".2.Reposition Files\")\n",
    "\n",
    "# Extract values for Tree from Graph\n",
    "Tree_Array = [];\n",
    "Tree_label = [];\n",
    "\n",
    "for f in fArr:\n",
    "    x = Gr.node[f][\"x\"]\n",
    "    y = Gr.node[f][\"y\"]\n",
    "    lab = Gr.node[f][\"label\"]\n",
    "    Tree_Array.append([x,y])\n",
    "    Tree_label.append(lab)\n",
    "\n",
    "tree = KDTree(Tree_Array)\n",
    "\n",
    "\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(text, test_k, single_value_only=True):\n",
    "    sumx=0\n",
    "    sumy=0\n",
    "    sumw=0\n",
    "    pred_lab = dict()\n",
    "    \n",
    "    res = FileToGraph(['file://file/name.ext', text])\n",
    "    for r in res:\n",
    "        ts = r.split('\\t')\n",
    "        t = ts[1]\n",
    "        w = float(ts[2])\n",
    "        \n",
    "        if t in tArr:\n",
    "            sumw+=w\n",
    "            # Get the x,y of the term from the model graph\n",
    "            sumx+=float(Gr.node[t][\"x\"])*w\n",
    "            sumy+=float(Gr.node[t][\"y\"])*w\n",
    "\n",
    "    if sumw > 0:\n",
    "        # Get the centroid\n",
    "        x = sumx/sumw\n",
    "        y = sumy/sumw\n",
    "        # Query the tree for nearest neighbor\n",
    "        dist,ind = tree.query([[x,y]], k=test_k)\n",
    "        for i in ind[0]:\n",
    "            if Tree_label[i] in pred_lab:\n",
    "                pred_lab[Tree_label[i]] += 1\n",
    "            else:\n",
    "                pred_lab[Tree_label[i]] = 1\n",
    "\n",
    "    else:\n",
    "        pred_lab[\"UNK\"] = 1\n",
    "    \n",
    "    # Set the return value to be the highest matched label\n",
    "    rVal = sorted(pred_lab.items(), key=operator.itemgetter(1), reverse=True)[0][0]\n",
    "    if single_value_only==False:\n",
    "        # Return the entire result dictionary\n",
    "        rVal = pred_lab\n",
    "        for l in pred_lab:\n",
    "            pred_lab[l] = float(pred_lab[l])/float(test_k)\n",
    "    \n",
    "    return rVal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sport': 1.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Runs and wickets from Australian paceman James Pattinson '\n",
    "text += 'and England''s Stuart Broad put Nottinghamshire firmly in command at Leicestershire. '\n",
    "text += 'Ben Raine (6-66) had helped reduce the visitors to 167-7, '\n",
    "text += 'still 84 runs adrift of the hosts'' first-innings 251. '\n",
    "text += 'But Broad (52) and Pattinson (89) added 122 for the eighth wicket to help Notts to a 78-run lead. '\n",
    "text += 'Luke Fletcher then piled in with three wickets as the hosts slumped to 51-6 at the close. '\n",
    "text += 'Making his Notts debut, Pattinson hit 14 boundaries and two sixes '\n",
    "text += 'in his 108-ball knock, and then had opener Harry Dearden caught behind '\n",
    "text += 'by Chris Read with the ninth ball of Leicestershire''s second innings. '\n",
    "text += 'Fletcher replaced Broad, who trapped Paul Horton lbw for two, and '\n",
    "text += 'produced an excellent spell of line and length to dismantle '\n",
    "text += 'Leicestershire''s middle order, putting Notts on the brink of victory. '\n",
    "predict(text, 17, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Predict against set of files \n",
    "# LOCAL PYTHON IMPLEMENTATION:\n",
    "\n",
    "def TestTextFiles(p, ext, k):\n",
    "    Actual = []\n",
    "    Pred = []\n",
    "    Labels = []\n",
    "    hasReturn = False\n",
    "    \n",
    "    # If this is a folder, we need to append ** for glob\n",
    "    if(os.path.isdir(p)==True):\n",
    "        p+=\"\\\\**\"\n",
    "    \n",
    "    f_list = glob2.glob(p)\n",
    "    \n",
    "    # Get all the (files - one file at a time)\n",
    "    for f in f_list:\n",
    "        \n",
    "        bOK = True\n",
    "        if len(ext) > 0:\n",
    "            if f.endswith(ext):\n",
    "                bOK = True\n",
    "            else:\n",
    "                bOK = False\n",
    "        \n",
    "        if bOK==True and os.path.isfile(f)==True:\n",
    "            label = os.path.basename(os.path.dirname(f))\n",
    "            contents = Path(f).read_text()\n",
    "            pred = predict(contents, k, True)\n",
    "            \n",
    "            Actual.append(label)\n",
    "            Pred.append(pred)\n",
    "            \n",
    "            if not label in Labels:\n",
    "                Labels.append(label)\n",
    "            if not pred in Labels:\n",
    "                Labels.append(pred)\n",
    "            \n",
    "            res = 0\n",
    "            if pred == label:\n",
    "                res = 1\n",
    "            \n",
    "            # There is at least one return\n",
    "            hasReturn = True\n",
    "                \n",
    "    if hasReturn == True:\n",
    "        accuracy = accuracy_score(Actual, Pred)\n",
    "        precision = precision_score(Actual, Pred, average='weighted') \n",
    "        recall = recall_score(Actual, Pred, average='weighted')\n",
    "        f1 = f1_score(Actual, Pred, average='weighted') \n",
    "\n",
    "        cm = ConfusionMatrix(Actual, Pred, Labels)\n",
    "    else:\n",
    "        accuracy = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "        cm = None\n",
    "        \n",
    "    return cm, precision, recall, f1, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      business  entertainment  politics  sport  tech  __all__\n",
      "Actual                                                                \n",
      "business            140              1         1      0     5      147\n",
      "entertainment        15             89        17      0     0      121\n",
      "politics              6              3       123      1     1      134\n",
      "sport                 0              0         3    155     2      160\n",
      "tech                  2              0         2      0   110      114\n",
      "__all__             163             93       146    156   118      676\n",
      "\n",
      "Accuracy\t0.912721893491\n",
      "Precision\t0.917439828418\n",
      "Recall\t0.912721893491\n",
      "F1\t0.911557837844\n",
      "Done in 14.957s.\n"
     ]
    }
   ],
   "source": [
    "#### Run Test\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "cm, p, r, f1, a = TestTextFiles(_path_to_test_files,'txt',13)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 400)\n",
    "\n",
    "print(cm)\n",
    "print(\"\")\n",
    "print(\"Accuracy\\t\" + str(a))\n",
    "print(\"Precision\\t\" + str(p))\n",
    "print(\"Recall\\t\" + str(r))\n",
    "print(\"F1\\t\" + str(f1))\n",
    "\n",
    "print(\"Done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "filename = _path_to_output + '\\\\' + _set_name + '_cm.txt'\n",
    "myfile = open(filename, 'w')\n",
    "\n",
    "myfile.write(_set_name + \"\\n\\n\")\n",
    "myfile.write(str(cm))\n",
    "myfile.write(\"\\n\")\n",
    "myfile.write(\"\\nAccuracy :\\t\" + str(a))\n",
    "myfile.write(\"\\nPrecision:\\t\" + str(p))\n",
    "myfile.write(\"\\nRecall   :\\t\" + str(r))\n",
    "myfile.write(\"\\nF-Measure:\\t\" + str(f1))\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
